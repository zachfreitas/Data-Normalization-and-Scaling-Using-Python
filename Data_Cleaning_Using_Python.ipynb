{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Data cleaning is the process of identifying and correcting or removing errors, inconsistencies, and irrelevant information from a dataset. The goal of data cleaning is to prepare the data for further analysis, modeling, or visualization by ensuring that it is accurate, consistent, and relevant. \n",
    "\n",
    "Data cleaning is an important step in the data science process because the quality of the data has a direct impact on the accuracy and validity of the results obtained from any further analysis or modeling. If the data is not cleaned, errors, inconsistencies, or irrelevant information may lead to incorrect or misleading results. For example, if a dataset contains missing values, the results of a statistical analysis may be biased or incorrect. If a dataset contains inconsistent or incorrect data types, the results of a machine learning model may be compromised. These are some examples of why data cleaning is an important, and perhaps the most important step for the success of any analytics project.\n",
    "\n",
    "In this module, we will cover the following topics:\n",
    "\n",
    "I. Missing data: A value that represents the absence of a valid value.\n",
    "II. Duplicate data: It refers to identical or nearly identical records in a database or dataset.\n",
    "III. Inconsistent data: It refers to the situation where data is stored in different or inconsistent formats within the same dataset.\n",
    "\n",
    "## Learning Objectives\n",
    "In this module, the learners will:\n",
    "\n",
    "* Identify missing values in a dataset\n",
    "* Apply techniques such as imputation for handling missing values\n",
    "* Identify and handle duplicate data in a dataset\n",
    "* Understand the importance of consistent data formats\n",
    "* Standardize data types and values in a given dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Breast Cancer dataset: This is a widely used dataset in the field of medical research and machine learning. This dataset provides information about breast cancer patients, including their demographic information, medical history, and the characteristics of their tumors. \n",
    "\n",
    "In terms of data cleaning, the dataset may contain missing values or erroneous data that need to be addressed before any analysis can be performed. Some of the common data cleaning techniques that can be applied to this dataset include identifying and handling missing values, checking for duplicates, and removing any irrelevant columns.\n",
    "\n",
    "The columns in this dataset are:\n",
    "\n",
    "* patient_id: This column contains a unique identification number assigned to each patient in the dataset.\n",
    "* clump_thickness: This column represents the thickness of the tumor in the range of 1 to 10.\n",
    "* cell_size_uniformity: This column represents the uniformity in size of tumor cells in the range of 1 to 10.\n",
    "* cell_shape_uniformity: This column represents the uniformity in shape of tumor cells in the range of 1 to 10.\n",
    "* marginal_adhesion: This column represents the level of adhesion of tumor cells to the surrounding tissue in the range of 1 to 10.\n",
    "* single_ep_cell_size: This column represents the size of the tumor's epithelial cells in the range of 1 to 10.\n",
    "* bare_nuclei: This column represents the presence or absence of a nucleus in the tumor cells. It contains values * * ranging from 1 to 10, where 1 represents the absence of a nucleus and 10 represents the presence of a nucleus.\n",
    "* bland_chromatin: This column represents the uniformity of the chromatin material within the tumor cells, ranging from 1 to 10.\n",
    "* normal_nucleoli: This column represents the normalcy of the nucleoli within the tumor cells, ranging from 1 to 10.\n",
    "* mitoses: This column represents the level of mitosis (cell division) within the tumor cells, ranging from 1 to 10.\n",
    "* class: This column contains the diagnosis of the tumor as either \"benign\" or \"malignant\".\n",
    "* doctor_name: This column contains the name of the doctor who diagnosed the tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data\n",
    "What is missing data?\n",
    "In real-world datasets, missing data is common and can occur for a variety of reasons such as errors in data collection, data entry, or processing. Missing values can create problems for data analysis and machine learning algorithms, as they may lead to biased or incorrect results. Therefore, handling missing values is an essential step in data preprocessing. \n",
    "\n",
    "In Python, a missing value, also known as a \"null value\" or \"not a number\" (NaN), is a value that represents the absence of a valid value. This can occur in many ways, such as when data is missing from a database, when a value cannot be calculated, or when a value is not entered for a specific data point. In Python, the standard way to represent missing values is by using the float type and the special constant NumPy.nan from the NumPy library.\n",
    "\n",
    "## Why is it important?\n",
    "Handling missing values is important for several reasons:\n",
    "\n",
    "* Missing values can impact the accuracy of data analysis and machine learning algorithms: If a missing value is not handled properly, it can lead to biased or incorrect results. For example, calculating the mean of a dataset with * * missing values will not accurately reflect the true mean if the missing values are not handled properly.\n",
    "* Missing values can lead to computational errors: Many machine learning algorithms and statistical models cannot handle missing values and will produce errors when encountering them. This can lead to incorrect results or even prevent the algorithm from running altogether.\n",
    "* Missing values can reduce the sample size and decrease the power of statistical tests: If a large portion of a dataset contains missing values, the sample size used for analysis will be reduced, potentially leading to lower statistical power and increased Type 2 errors.\n",
    "* Missing values can impact the interpretability of results: If missing values are not handled properly, they can cause misinterpretation of results and mislead decision-making.\n",
    "\n",
    "Let's look at a few ways to deal with missing data in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the Missing Values\n",
    "There are several techniques to deal with missing values. In this section, we will focus on the deletion technique, which involves dropping rows or columns with missing values. We will provide examples of how to implement these techniques using the Pandas library in Python.\n",
    "\n",
    "### Dropping the Rows\n",
    "One approach to handling missing values in a dataset is to remove entire rows that contain any missing values. By dropping rows with missing values, we effectively eliminate the observations that have incomplete data. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the original DataFrame:\n",
      " patient_id               0\n",
      "clump_thickness          1\n",
      "cell_size_uniformity     1\n",
      "cell_shape_uniformity    0\n",
      "marginal_adhesion        0\n",
      "single_ep_cell_size      0\n",
      "bare_nuclei              2\n",
      "bland_chromatin          4\n",
      "normal_nucleoli          1\n",
      "mitoses                  0\n",
      "class                    0\n",
      "doctor_name              0\n",
      "dtype: int64 \n",
      "\n",
      "Size of DataFrame before deletion:\n",
      "(699, 12)\n",
      "Size of DataFrame after deletion:\n",
      "(690, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "# Find missing values \n",
    "print(\"Missing values in the original DataFrame:\\n\", df.isnull().sum(), \"\\n\") \n",
    "\n",
    "# Remove rows with missing values \n",
    "df_cleaned = df.dropna() \n",
    "\n",
    "# Print the size of the DataFrame before deletion \n",
    "print(\"Size of DataFrame before deletion:\") \n",
    "print(df.shape) \n",
    "\n",
    "# Print the size of the modified DataFrame \n",
    "print(\"Size of DataFrame after deletion:\") \n",
    "print(df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code starts by importing the Pandas library, which provides powerful data manipulation capabilities. We then load the dataset with missing values using the read_csv function. Adjust the file path and name according to your dataset.\n",
    "\n",
    "To identify the missing values in the DataFrame, we use the isnull().sum() method, which returns the count of missing values for each column. By applying the dropna() function to the DataFrame df, we remove rows that contain any missing values. The resulting cleaned DataFrame is stored in df_cleaned.\n",
    "\n",
    "Before deletion, we print the size of the DataFrame using the shape attribute. This gives us an understanding of the number of rows and columns in the original dataset. After deletion, we print the size of the modified DataFrame to observe the impact of removing rows with missing values.\n",
    "\n",
    "### Dropping the Columns\n",
    "Another approach to handling missing values is to remove entire columns that contain any missing values. By dropping columns with missing values, we eliminate the features or variables that have incomplete data. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in the original DataFrame:\n",
      " patient_id               0\n",
      "clump_thickness          1\n",
      "cell_size_uniformity     1\n",
      "cell_shape_uniformity    0\n",
      "marginal_adhesion        0\n",
      "single_ep_cell_size      0\n",
      "bare_nuclei              2\n",
      "bland_chromatin          4\n",
      "normal_nucleoli          1\n",
      "mitoses                  0\n",
      "class                    0\n",
      "doctor_name              0\n",
      "dtype: int64 \n",
      "\n",
      "Size of DataFrame before deletion:\n",
      "(699, 12)\n",
      "Size of DataFrame after deletion:\n",
      "(699, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load a dataset with missing values \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "# Find missing values \n",
    "print(\"Missing values in the original DataFrame:\\n\", df.isnull().sum(), \"\\n\") \n",
    "\n",
    "# Remove columns with missing values \n",
    "df_cleaned = df.dropna(axis=1) \n",
    "\n",
    "# Print the size of the DataFrame before deletion \n",
    "print(\"Size of DataFrame before deletion:\") \n",
    "print(df.shape) \n",
    "\n",
    "# Print the size of the modified DataFrame \n",
    "print(\"Size of DataFrame after deletion:\") \n",
    "print(df_cleaned.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we start by importing the Pandas library and loading the dataset with missing values using the read_csv function.\n",
    "\n",
    "We use the isnull().sum() method to identify the missing values in the DataFrame, which gives us the count of missing values for each column. To remove columns that contain any missing values, we apply the dropna(axis=1) function to the DataFrame df. The resulting cleaned DataFrame is stored in df_cleaned.\n",
    "\n",
    "Before deletion, we print the size of the DataFrame using the shape attribute to get an idea of the number of rows and columns in the original dataset. After deletion, we print the size of the modified DataFrame to observe the impact of removing columns with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Tendency Imputation \n",
    "Imputation involves replacing missing values in a dataset with estimated values, and it serves as a technique for handling missing data. Among the various methods used for imputation, one common approach is to substitute the missing values with the mean, median, or mode of the available observed values. This allows for a more complete and reliable dataset for further analysis or modeling.\n",
    "\n",
    "### Imputation Using Mean\n",
    "Mean imputation involves replacing missing values with the mean of the observed values. This method assumes that the missing values are missing at random and that the mean of the observed values is a good estimate for the missing values. Here is an example of mean imputation in Python using the Pandas library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'bland_chromatin' before imputation:\n",
      "  4 \n",
      "\n",
      "Missing values in 'bland_chromatin' after imputation:\n",
      "  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load a dataset with missing values \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "# Find missing values \n",
    "print(\"Missing values in 'bland_chromatin' before imputation:\\n \", df['bland_chromatin'].isnull().sum(),\"\\n\") \n",
    "\n",
    "# Impute missing values with mean \n",
    "df['bland_chromatin'].fillna(df['bland_chromatin'].mean(), inplace=True) \n",
    "\n",
    "# Find missing values after imputation \n",
    "print(\"Missing values in 'bland_chromatin' after imputation:\\n \",  \n",
    "df['bland_chromatin'].isnull().sum(),\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet utilizes the Pandas library to handle missing values in the 'bland_chromatin' column of a dataset. The code determines the number of missing values present in the 'bland_chromatin' column by using the isnull().sum() method. The isnull() function returns a boolean mask indicating missing values, and the sum() function counts the number of True values (i.e., missing values). This provides insights into the count of missing values prior to any imputation.\n",
    "\n",
    "To address the missing values, the code employs the fillna() method on the 'bland_chromatin' column. By setting inplace=True, the DataFrame is modified directly, and missing values are replaced with the mean value of the 'bland_chromatin' column, obtained using the mean() method. Subsequently, the code checks the count of missing values in the 'bland_chromatin' column again using isnull().sum(), thus revealing the count of missing values subsequent to imputation.\n",
    "\n",
    "### Imputation Using Median\n",
    "Median imputation involves replacing missing values with the median of the observed values. This method is similar to mean imputation, but the median is used instead of the mean. Since the median is less sensitive to extreme values, this method can be more robust to outliers than mean imputation. Here is an example of median imputation in Python using the Pandas library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'bland_chromatin' before imputation:\n",
      "  4 \n",
      "\n",
      "Missing values in 'bland_chromatin' after imputation:\n",
      "  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load a dataset with missing values \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "# Find missing values \n",
    "print(\"Missing values in 'bland_chromatin' before imputation:\\n \", df['bland_chromatin'].isnull().sum(),\"\\n\") \n",
    "\n",
    "# Impute missing values with median \n",
    "df['bland_chromatin'].fillna(df['bland_chromatin'].median(), inplace=True) \n",
    "\n",
    "\n",
    "# Find missing values after imputation \n",
    "print(\"Missing values in 'bland_chromatin' after imputation:\\n \",  \n",
    "df['bland_chromatin'].isnull().sum(),\"\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code determines the count of missing values in the 'bland_chromatin' column using the isnull().sum() method, providing an overview of missing values before any imputation.\n",
    "\n",
    "To handle the missing values, the code utilizes the fillna() method on the 'bland_chromatin' column. By setting inplace=True, the DataFrame is modified directly, and the missing values are replaced with the median value of the 'bland_chromatin' column, obtained using the median() method. The code then checks the count of missing values in the 'bland_chromatin' column again using isnull().sum(), allowing for an assessment of the count of missing values after the imputation process.\n",
    "\n",
    "### Imputation Using Mode\n",
    "Mean and median imputation only work on numeric data, for non-numeric data we need to look at other methods. One such method is using the mode instead of mean or median. Mode imputation involves replacing missing values with the mode of the observed values. The mode is the most frequently occurring value in the dataset. This method is commonly used for categorical data, where the mode represents the most common category, but it can also be used for other types of data when appropriate. Here is an example of mode imputation in Python using the Pandas library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'bland_chromatin' before imputation:\n",
      "  4 \n",
      "\n",
      "Missing values in 'bland_chromatin' after imputation:\n",
      "  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    " \n",
    "\n",
    "# Load a dataset with missing values \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    " \n",
    "\n",
    "# Find missing values \n",
    "print(\"Missing values in 'bland_chromatin' before imputation:\\n \", df['bland_chromatin'].isnull().sum(),\"\\n\") \n",
    " \n",
    "\n",
    "# Impute missing values with mode \n",
    "df['bland_chromatin'].fillna(df['bland_chromatin'].mode()[0], inplace=True) \n",
    " \n",
    "\n",
    "# Find missing values after imputation \n",
    "print(\"Missing values in 'bland_chromatin' after imputation:\\n \",  \n",
    "df['bland_chromatin'].isnull().sum(),\"\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks for missing values in the 'bland_chromatin' column using the isnull().sum() method. To handle the missing values, the code uses the fillna() method on the 'bland_chromatin' column. The fillna() method replaces missing values with a specified value or strategy. In this case, the mode (most frequent value) of the 'bland_chromatin' column is calculated using df['bland_chromatin'].mode()[0]. The mode is obtained by calling the mode() method on the column, which returns a Series of mode values (it could be multiple modes if there are ties). By accessing the first value [0], we ensure that only the most frequent value is used for imputation. The inplace=True argument ensures that the DataFrame is modified in place, without the need for assignment.\n",
    "\n",
    "After the missing values are imputed, the code checks for missing values in the 'bland_chromatin' column again using the same isnull().sum() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicate Data\n",
    "\n",
    "## What is duplicate data?\n",
    "Duplicate data refers to identical or nearly identical records in a database or dataset. These duplicate records can arise for various reasons, such as human error, technical issues, or multiple sources of data being merged without identifying and removing duplicate records in a dataset or database.\n",
    "\n",
    "## Why is it important?\n",
    "Duplicate data can lead to a variety of problems, such as inflating the size of the database, causing confusion when analyzing the data, and introducing inconsistencies in the data.\n",
    "\n",
    "Imagine a healthcare organization that maintains a database of patient records. Each record contains information such as the patient's name, address, date of birth, and medical history. Over time, multiple records might be created for the same patient due to various reasons, such as data entry errors, system glitches, or merging of records from different sources.\n",
    "\n",
    "Now, imagine that a physician needs to look up a patient's medical history to determine the best course of treatment. If the patient has multiple records in the database, the physician might not be able to accurately determine which record contains the most up-to-date and accurate information. As a result, the physician might make an incorrect diagnosis or prescribe the wrong medication, leading to potential harm to the patient.\n",
    "\n",
    "By removing duplicates, the healthcare organization can ensure that each patient has only one accurate and up-to-date record in the database. This helps ensure that healthcare providers have access to the most accurate and reliable information when making critical decisions about patient care.\n",
    "\n",
    "This is just one example of why dealing with duplicate data is important. In general, it is essential for organizations to identify and remove duplicates in their data to ensure the accuracy and reliability of the data, avoid confusion and inconsistencies, and make informed decisions based on the data.\n",
    "\n",
    "## Detecting Duplicates\n",
    "Duplicate data is a common issue that can arise due to various factors, including data entry errors, system malfunctions, or data integration processes. Addressing duplicate data is an essential aspect of data cleaning. However, before dealing with duplicate data, it is crucial to first identify the duplicates. In Python, there are multiple techniques available to mark duplicated data. In this section, we will explore some of the main techniques used for identifying duplicate data.\n",
    "\n",
    "### Identify Duplicates Based on All Columns\n",
    "One technique for identifying duplicate data involves considering all columns in the dataset. This approach compares the entire rows of the dataset to identify exact duplicates, where all column values match.\n",
    "\n",
    "By examining all columns simultaneously, this technique provides a comprehensive assessment of duplicated records across the entire dataset. It helps in identifying instances where the same data has been entered multiple times or when there are repeated observations in the dataset. Here is an example of how you can identify duplicates based on all columns in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    698\n",
      "True       1\n",
      "dtype: int64\n",
      "     patient_id  clump_thickness  cell_size_uniformity  cell_shape_uniformity  \\\n",
      "258     1198641              3.0                   1.0                      1   \n",
      "\n",
      "     marginal_adhesion  single_ep_cell_size bare_nuclei  bland_chromatin  \\\n",
      "258                  1                    2           1              3.0   \n",
      "\n",
      "     normal_nucleoli  mitoses   class doctor_name  \n",
      "258              1.0        1  benign     Dr. Lee  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load the dataset \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "# Identify duplicates based on all columns \n",
    "duplicates = df.duplicated() \n",
    "\n",
    "# Print out the duplicate counts  \n",
    "print(duplicates.value_counts()) \n",
    "\n",
    "# Print the duplicate row(s) \n",
    "print(df[duplicates]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code begins by loading the dataset \"breast_cancer_data.csv\" into a Pandas DataFrame called df. The next step involves identifying duplicate rows based on all columns using the duplicated() function, which returns a boolean series indicating whether each row is a duplicate or not. \n",
    "\n",
    "Next, the code proceeds to print the counts of duplicate values using the value_counts() function applied to the boolean series duplicates. This provides a summary of the number of occurrences for each unique value in the series, indicating the count of duplicates (True) and non-duplicates (False). Lastly, the duplicate rows are printed using the print() function, showing the specific rows that are considered duplicates based on the boolean series duplicates.\n",
    "\n",
    "### Identify Duplicates Based on Specific Columns\n",
    "In addition to identifying duplicates based on all columns, it is also common to focus on specific columns when detecting duplicate data. This technique allows us to determine if there are any repeated or highly similar records based on a subset of columns that are considered relevant for identifying duplicates.\n",
    "\n",
    "By selecting specific columns for duplicate identification, we can tailor the analysis to focus on the attributes that are most important for determining duplicity. This approach is particularly useful when certain columns, such as unique identifiers or key variables, play a crucial role in identifying and distinguishing records. Here is an example of how you can identify duplicates based on specific columns in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    683\n",
      "True      16\n",
      "dtype: int64\n",
      "     patient_id  clump_thickness  cell_size_uniformity  cell_shape_uniformity  \\\n",
      "208     1218860              1.0                   1.0                      1   \n",
      "253     1100524              6.0                  10.0                     10   \n",
      "254     1116116              9.0                  10.0                     10   \n",
      "257     1182404              3.0                   1.0                      1   \n",
      "258     1198641              3.0                   1.0                      1   \n",
      "272      320675              3.0                   3.0                      5   \n",
      "322      733639              3.0                   1.0                      1   \n",
      "338      704097              1.0                   1.0                      1   \n",
      "393     1158247              1.0                   1.0                      1   \n",
      "443      734111              1.0                   1.0                      1   \n",
      "490     1115293              1.0                   1.0                      1   \n",
      "517     1320077              1.0                   1.0                      1   \n",
      "561     1321942              5.0                   1.0                      1   \n",
      "684      466906              1.0                   1.0                      1   \n",
      "690      654546              1.0                   1.0                      1   \n",
      "698      897471              4.0                   8.0                      8   \n",
      "\n",
      "     marginal_adhesion  single_ep_cell_size bare_nuclei  bland_chromatin  \\\n",
      "208                  1                    1           1              3.0   \n",
      "253                  2                    8          10              7.0   \n",
      "254                  1                   10           8              3.0   \n",
      "257                  1                    2           1              2.0   \n",
      "258                  1                    2           1              3.0   \n",
      "272                  2                    3          10              7.0   \n",
      "322                  1                    2           1              3.0   \n",
      "338                  1                    1           1              2.0   \n",
      "393                  1                    1           1              1.0   \n",
      "443                  1                    2           2              1.0   \n",
      "490                  1                    2           1              1.0   \n",
      "517                  1                    1           1              2.0   \n",
      "561                  1                    2           1              3.0   \n",
      "684                  1                    2           1              1.0   \n",
      "690                  3                    2           1              1.0   \n",
      "698                  5                    4           5             10.0   \n",
      "\n",
      "     normal_nucleoli  mitoses      class doctor_name  \n",
      "208              1.0        1     benign     Dr. Doe  \n",
      "253              3.0        3  malignant     Dr. Lee  \n",
      "254              3.0        1  malignant    Dr. Wong  \n",
      "257              1.0        1     benign     Dr. Doe  \n",
      "258              1.0        1     benign     Dr. Lee  \n",
      "272              1.0        1  malignant   Dr. Smith  \n",
      "322              1.0        1     benign     Dr. Doe  \n",
      "338              1.0        1     benign    Dr. Wong  \n",
      "393              1.0        1     benign     Dr. Doe  \n",
      "443              1.0        1     benign     Dr. Doe  \n",
      "490              1.0        1     benign     Dr. Doe  \n",
      "517              1.0        1     benign     Dr. Doe  \n",
      "561              1.0        1     benign     Dr. Lee  \n",
      "684              1.0        1     benign    Dr. Wong  \n",
      "690              1.0        1     benign     Dr. Doe  \n",
      "698              4.0        1  malignant    Dr. Wong  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load the dataset \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "# Specify the columns for duplicate identification \n",
    "columns_to_check = ['patient_id', 'clump_thickness', 'cell_size_uniformity'] \n",
    "\n",
    "# Identify duplicates based on specific columns \n",
    "duplicates = df.duplicated(subset=columns_to_check) \n",
    "\n",
    "\n",
    "# Print out the duplicate counts \n",
    "print(duplicates.value_counts()) \n",
    "\n",
    "# Print the duplicate rows \n",
    "print(df[duplicates]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code specifies the columns to be considered for duplicate identification by creating a list columns_to_check containing the column names 'patient_id', 'clump_thickness', and 'cell_size_uniformity'. These columns are relevant in determining the duplicity of records in the dataset, capturing both patient identifiers and features related to the clump thickness and cell size uniformity.\n",
    "\n",
    "The code then uses the duplicated() function on the DataFrame df, specifying the 'subset' parameter as columns_to_check. This creates a boolean mask duplicates, which indicates True for rows that are duplicates based on the specified columns and False for non-duplicate rows.\n",
    "\n",
    "To provide a summary of the duplicate counts, the code applies the value_counts() function to the boolean mask duplicates. This counts the occurrences of True and False values, indicating the number of duplicates and non-duplicates, respectively. Finally, the code displays the duplicate rows by using the boolean mask duplicates to index the DataFrame df.\n",
    "\n",
    "### Mark Duplicates Using the Keep Parameter\n",
    "Marking duplicates using the keep parameter is a technique used to identify and label duplicate rows within a dataset. The keep parameter allows for customizing which duplicate values should be marked or considered as duplicates.\n",
    "\n",
    "By default, when marking duplicates, the first occurrence of a duplicate value is considered as non-duplicate, while all subsequent occurrences are marked as duplicates. However, the keep parameter provides different options to modify this behavior:\n",
    "\n",
    "* keep='first': This is the default value and keeps the first occurrence of a duplicated value as non-duplicate, marking all subsequent occurrences as duplicates. \n",
    "* keep='last': This option keeps the last occurrence of a duplicated value as non-duplicate, marking all previous occurrences as duplicates. \n",
    "* keep=False: Setting keep to False marks all occurrences of duplicate values as duplicates, considering none of them as non-duplicates\n",
    "\n",
    "By specifying the keep parameter, you can control how duplicates are marked and choose which occurrence(s) to consider as non-duplicates. This provides flexibility in handling duplicates based on specific requirements or analysis needs.\n",
    "\n",
    "Here is an example of how you can identify duplicates using the keep parameter in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     patient_id  clump_thickness  cell_size_uniformity  cell_shape_uniformity  \\\n",
      "689      654546              1.0                   1.0                      1   \n",
      "690      654546              1.0                   1.0                      1   \n",
      "691      695091              5.0                  10.0                     10   \n",
      "692      714039              3.0                   1.0                      1   \n",
      "693      763235              3.0                   1.0                      1   \n",
      "694      776715              3.0                   1.0                      1   \n",
      "695      841769              2.0                   1.0                      1   \n",
      "696      888820              5.0                  10.0                     10   \n",
      "697      897471              4.0                   8.0                      6   \n",
      "698      897471              4.0                   8.0                      8   \n",
      "\n",
      "     marginal_adhesion  single_ep_cell_size bare_nuclei  bland_chromatin  \\\n",
      "689                  1                    2           1              1.0   \n",
      "690                  3                    2           1              1.0   \n",
      "691                  5                    4           5              4.0   \n",
      "692                  1                    2           1              1.0   \n",
      "693                  1                    2           1              2.0   \n",
      "694                  1                    3           2              1.0   \n",
      "695                  1                    2           1              1.0   \n",
      "696                  3                    7           3              8.0   \n",
      "697                  4                    3           4             10.0   \n",
      "698                  5                    4           5             10.0   \n",
      "\n",
      "     normal_nucleoli  mitoses      class doctor_name  Duplicates  \n",
      "689              1.0        8     benign     Dr. Lee       False  \n",
      "690              1.0        1     benign     Dr. Doe        True  \n",
      "691              4.0        1  malignant    Dr. Wong       False  \n",
      "692              1.0        1     benign    Dr. Wong       False  \n",
      "693              1.0        2     benign     Dr. Lee       False  \n",
      "694              1.0        1     benign     Dr. Lee       False  \n",
      "695              1.0        1     benign   Dr. Smith       False  \n",
      "696             10.0        2  malignant     Dr. Lee       False  \n",
      "697              6.0        1  malignant     Dr. Lee       False  \n",
      "698              4.0        1  malignant    Dr. Wong        True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "# Load the dataset  \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\")  \n",
    "\n",
    "# Specify the columns for duplicate identification  \n",
    "columns_to_check = ['patient_id', 'clump_thickness', 'cell_size_uniformity']  \n",
    "\n",
    "# Mark duplicates using the keep parameter  \n",
    "duplicates = df.duplicated(subset=columns_to_check, keep='first')  \n",
    "\n",
    "# Mark the duplicates \n",
    "df['Duplicates'] = duplicates \n",
    "\n",
    "# Print the duplicate rows  \n",
    "print(df.tail(10))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the provided code, a list of column names, columns_to_check, is specified. These columns serve the purpose of identifying and marking duplicates within the dataset. By utilizing the duplicated() function, with the subset parameter set to columns_to_check and the keep parameter set to 'first', the code effectively flags the duplicate entries in the DataFrame df. \n",
    "\n",
    "To make the duplicates more visible, a new column 'Duplicates' is added to the DataFrame df, which contains the values of the duplicates series. Finally, the last 10 rows of the DataFrame are displayed using the tail() and the print() functions, allowing you to observe and analyze the marked duplicate rows along with the rest of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Duplicates\n",
    "After identifying duplicates using the techniques discussed earlier, the next step is to remove these duplicates from the dataset. Duplicate data can introduce inaccuracies and biases in the analysis, leading to erroneous insights and conclusions.\n",
    "\n",
    "By removing duplicates, you eliminate redundant information and streamline the dataset, making it more manageable and suitable for analysis. There are multiple methods available for removing duplicates, each with its own advantages and specific use cases. The choice of method depends on the characteristics of the dataset and the objectives of the analysis.\n",
    "\n",
    "### Dropping Duplicate Rows\n",
    "Dropping duplicate rows is a technique used to remove duplicate observations from a dataset. Duplicate rows can occur due to various reasons, such as data entry errors or merging multiple datasets. Removing these duplicates ensures the integrity and accuracy of the data, preventing biased or misleading analysis.\n",
    "\n",
    "Here is an example of dropping duplicate rows on the breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate row(s) in the DataFrame      patient_id  clump_thickness  cell_size_uniformity  cell_shape_uniformity  \\\n",
      "258     1198641              3.0                   1.0                      1   \n",
      "\n",
      "     marginal_adhesion  single_ep_cell_size bare_nuclei  bland_chromatin  \\\n",
      "258                  1                    2           1              3.0   \n",
      "\n",
      "     normal_nucleoli  mitoses   class doctor_name  \n",
      "258              1.0        1  benign     Dr. Lee  \n",
      "After removing duplicates from the DataFrame Empty DataFrame\n",
      "Columns: [patient_id, clump_thickness, cell_size_uniformity, cell_shape_uniformity, marginal_adhesion, single_ep_cell_size, bare_nuclei, bland_chromatin, normal_nucleoli, mitoses, class, doctor_name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries \n",
    "import pandas as pd \n",
    "import warnings   \n",
    " \n",
    "# Ignore warning messages to enhance readability \n",
    "warnings.filterwarnings('ignore')    \n",
    " \n",
    "# Load the breast cancer dataset \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "# Identify duplicates based on all columns  \n",
    "duplicates = df.duplicated()  \n",
    " \n",
    "# Print the duplicate row(s)  \n",
    "print('Duplicate row(s) in the DataFrame', df[duplicates])   \n",
    "\n",
    "# Remove the duplicate rows from the DataFrame in-place \n",
    "df.drop_duplicates(inplace=True) \n",
    "\n",
    "\n",
    "# Print the duplicate rows after removing duplicates from the DataFrame \n",
    "print('After removing duplicates from the DataFrame', df[duplicates]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the given code, the breast cancer dataset is loaded into a DataFrame named df. To enhance readability, the warnings library is imported, and warning messages are ignored using the filterwarnings() method.\n",
    "\n",
    "The duplicated() function is utilized to identify duplicate rows in the DataFrame by considering all columns. To display the duplicate rows, the print() function is used with the DataFrame df and the boolean index duplicates as parameters. This shows the rows that are considered duplicates based on the entire dataset.\n",
    "\n",
    "Next, the drop_duplicates() function is applied to the DataFrame to remove the duplicate rows in place, ensuring that only unique observations remain. By setting inplace=True, the DataFrame df is modified directly.\n",
    "\n",
    "Finally, the print() function is used again to show the duplicate rows after removing duplicates.\n",
    "* Keeping the first occurrence\n",
    "\n",
    "In certain scenarios, you may want to keep only the first occurrence of duplicated rows in a dataset and remove any subsequent duplicates. This approach ensures that you retain the earliest record of a duplicated entry while eliminating redundant information.\n",
    "\n",
    "Here's an example code that demonstrates keeping the first occurrence of duplicated rows using the drop_duplicates() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Dataset:\n",
      "   patient_id  clump_thickness  cell_size_uniformity  cell_shape_uniformity  \\\n",
      "0     1000025              5.0                   1.0                      1   \n",
      "1     1002945              5.0                   4.0                      4   \n",
      "2     1015425              3.0                   1.0                      1   \n",
      "3     1016277              6.0                   8.0                      8   \n",
      "4     1017023              4.0                   1.0                      1   \n",
      "\n",
      "   marginal_adhesion  single_ep_cell_size bare_nuclei  bland_chromatin  \\\n",
      "0                  1                    2           1              3.0   \n",
      "1                  5                    7          10              3.0   \n",
      "2                  1                    2           2              3.0   \n",
      "3                  1                    3           4              3.0   \n",
      "4                  3                    2           1              3.0   \n",
      "\n",
      "   normal_nucleoli  mitoses   class doctor_name  \n",
      "0              1.0        1  benign     Dr. Doe  \n",
      "1              2.0        1  benign   Dr. Smith  \n",
      "2              1.0        1  benign     Dr. Lee  \n",
      "3              7.0        1  benign   Dr. Smith  \n",
      "4              1.0        1  benign    Dr. Wong  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load the breast cancer dataset \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "# Keep the first occurrence of duplicates \n",
    "df_cleaned = df.drop_duplicates(keep='first') \n",
    "\n",
    "# Print the cleaned dataset \n",
    "print(\"\\nCleaned Dataset:\") \n",
    "print(df_cleaned.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the provided code, we use the drop_duplicates() function to eliminate duplicate rows while keeping only the first occurrence. By specifying keep='first', the function retains the first instance of each duplicated row and removes any subsequent duplicates.\n",
    "\n",
    "The cleaned dataset is stored in the DataFrame df_cleaned. It contains the original rows but without any duplicates beyond the first occurrence. Finally, we print the cleaned dataset to observe the result.\n",
    "\n",
    "* Keeping the last occurrence\n",
    "\n",
    "In certain cases, you may want to keep only the last occurrence of duplicated rows in a dataset and remove any previous duplicates. This approach ensures that you retain the most recent record of a duplicated entry while discarding redundant information.\n",
    "\n",
    "Here's an example code that demonstrates keeping the last occurrence of duplicated rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Dataset:\n",
      "   patient_id  clump_thickness  cell_size_uniformity  cell_shape_uniformity  \\\n",
      "0     1000025              5.0                   1.0                      1   \n",
      "1     1002945              5.0                   4.0                      4   \n",
      "2     1015425              3.0                   1.0                      1   \n",
      "3     1016277              6.0                   8.0                      8   \n",
      "4     1017023              4.0                   1.0                      1   \n",
      "\n",
      "   marginal_adhesion  single_ep_cell_size bare_nuclei  bland_chromatin  \\\n",
      "0                  1                    2           1              3.0   \n",
      "1                  5                    7          10              3.0   \n",
      "2                  1                    2           2              3.0   \n",
      "3                  1                    3           4              3.0   \n",
      "4                  3                    2           1              3.0   \n",
      "\n",
      "   normal_nucleoli  mitoses   class doctor_name  \n",
      "0              1.0        1  benign     Dr. Doe  \n",
      "1              2.0        1  benign   Dr. Smith  \n",
      "2              1.0        1  benign     Dr. Lee  \n",
      "3              7.0        1  benign   Dr. Smith  \n",
      "4              1.0        1  benign    Dr. Wong  \n"
     ]
    }
   ],
   "source": [
    "# Load the breast cancer dataset \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    " \n",
    "# Keep the last occurrence of duplicates \n",
    "df_cleaned = df.drop_duplicates(keep='last') \n",
    "\n",
    "  \n",
    "# Print the cleaned dataset \n",
    "print(\"\\nCleaned Dataset:\") \n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the provided code, we use the drop_duplicates() function to remove duplicate rows while keeping only the last occurrence. By specifying keep='last', the function retains the most recent instance of each duplicated row and eliminates any previous duplicates.\n",
    "\n",
    "### Removing Duplicates based on Specific Columns\n",
    "Removing duplicates based on specific columns involves identifying duplicates only within those columns and removing them from the dataset. This technique is useful when you want to focus on specific attributes or when duplicates in certain columns are more critical to address.\n",
    "\n",
    "Here's the code snippet to remove duplicates based on specific columns using the breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     patient_id  clump_thickness  cell_size_uniformity  cell_shape_uniformity  \\\n",
      "687      566346              3.0                   1.0                      1   \n",
      "688      603148              4.0                   1.0                      1   \n",
      "689      654546              1.0                   1.0                      1   \n",
      "691      695091              5.0                  10.0                     10   \n",
      "692      714039              3.0                   1.0                      1   \n",
      "693      763235              3.0                   1.0                      1   \n",
      "694      776715              3.0                   1.0                      1   \n",
      "695      841769              2.0                   1.0                      1   \n",
      "696      888820              5.0                  10.0                     10   \n",
      "697      897471              4.0                   8.0                      6   \n",
      "\n",
      "     marginal_adhesion  single_ep_cell_size bare_nuclei  bland_chromatin  \\\n",
      "687                  1                    2           1              2.0   \n",
      "688                  1                    2           1              1.0   \n",
      "689                  1                    2           1              1.0   \n",
      "691                  5                    4           5              4.0   \n",
      "692                  1                    2           1              1.0   \n",
      "693                  1                    2           1              2.0   \n",
      "694                  1                    3           2              1.0   \n",
      "695                  1                    2           1              1.0   \n",
      "696                  3                    7           3              8.0   \n",
      "697                  4                    3           4             10.0   \n",
      "\n",
      "     normal_nucleoli  mitoses      class doctor_name  \n",
      "687              3.0        1     benign     Dr. Lee  \n",
      "688              1.0        1     benign   Dr. Smith  \n",
      "689              1.0        8     benign     Dr. Lee  \n",
      "691              4.0        1  malignant    Dr. Wong  \n",
      "692              1.0        1     benign    Dr. Wong  \n",
      "693              1.0        2     benign     Dr. Lee  \n",
      "694              1.0        1     benign     Dr. Lee  \n",
      "695              1.0        1     benign   Dr. Smith  \n",
      "696             10.0        2  malignant     Dr. Lee  \n",
      "697              6.0        1  malignant     Dr. Lee  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load the breast cancer dataset \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "# Specify the columns for duplicate identification \n",
    "columns_to_check = ['patient_id', 'clump_thickness', 'cell_size_uniformity'] \n",
    "\n",
    "# Remove duplicates based on specific columns \n",
    "df_cleaned = df.drop_duplicates(subset=columns_to_check) \n",
    "\n",
    "# Print the cleaned dataset \n",
    "print(df_cleaned.tail(10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we specify the columns we want to consider for duplicate identification in the columns_to_check list. Next, we use the drop_duplicates() function with the subset parameter set to columns_to_check to remove duplicates based on those specific columns. The resulting cleaned dataset is stored in the df_cleaned DataFrame.\n",
    "\n",
    "By printing the last 10 rows of the cleaned dataset using tail(10), we can observe the dataset without duplicates based on the specified columns. This approach allows for the targeted removal of duplicates, focusing on the chosen attributes while preserving other information in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is inconsistent data?\n",
    "Inconsistent data formats refer to the situation where data is stored in different or inconsistent formats within the same dataset. Inconsistent data formats can also refer to differences in the formatting of similar data, such as different date or time formats, different currency formats, or different units of measurement.\n",
    "\n",
    "## Why is it important?\n",
    "Inconsistencies can make it difficult to correctly interpret data, especially if the data is intended for analysis or comparison. For example, consider a dataset that contains sales information, where the currency used for sales is US dollars in some cases and British pounds in others. This inconsistent format can create problems when trying to perform operations on the data, such as finding the total amount of revenue generated from sales.\n",
    "\n",
    "Inconsistent data formats can arise for many reasons, including manual data entry errors, differences in data collection methods, and differences in the way data are stored or processed by different systems. Regardless of the cause, addressing and resolving inconsistent data formats is an important step in the data cleaning and preparation process, as it ensures that the data is usable and meaningful for analysis and other purposes.\n",
    "\n",
    "## Data Standardization\n",
    "Data standardization is a crucial step in the data cleaning process. Inconsistent data formats and scales can lead to issues during analysis and modeling. Data standardization ensures that the data is on a common scale, follows a consistent format, and is suitable for further analysis or modeling tasks.\n",
    "\n",
    "By applying data standardization techniques, you can improve data quality, enhance comparability across variables, and reduce biases or discrepancies caused by inconsistent data formats.\n",
    "\n",
    "### Standardizing Data Types\n",
    "Standardizing data types is an important step in data cleaning to ensure consistency and compatibility across different data sources. In this section, we focus on standardizing the data types of specific columns in the breast cancer dataset. This step ensures that the data is represented in the appropriate format for further analysis and processing. Here is the code that checks the data types of all columns in the breast cancer dataset using the dtypes function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types of Columns: \n",
      "patient_id                 int64\n",
      "clump_thickness          float64\n",
      "cell_size_uniformity     float64\n",
      "cell_shape_uniformity      int64\n",
      "marginal_adhesion          int64\n",
      "single_ep_cell_size        int64\n",
      "bare_nuclei               object\n",
      "bland_chromatin          float64\n",
      "normal_nucleoli          float64\n",
      "mitoses                    int64\n",
      "class                     object\n",
      "doctor_name               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load the breast cancer dataset \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "# Check the data types of all columns \n",
    "print(\"Data Types of Columns: \") \n",
    "print(df.dtypes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, it can be observed that the \"bare_nuclei\" column is currently classified as an object data type. Considering the context of the data and the expected range of values (1 to 10), it is more appropriate for the \"bare_nuclei\" column to be of a numeric data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Values in the 'bare' column:\n",
      "['1' '10' '2' '4' '3' '9' '7' '?' '5' '8' '6' nan]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load the breast cancer dataset \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "\n",
    "# Check the unique values in the \"bare\" column \n",
    "unique_values = df['bare_nuclei'].unique() \n",
    "print(\"Unique Values in the 'bare' column:\") \n",
    "print(unique_values) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the unique() function is used to obtain the unique values in the \"bare_nuclei\" column. By inspecting the unique values, you can determine if there are any non-numeric values, such as the \"?\" symbol, in the \"bare_nuclei\" column before proceeding with the conversion to a numeric data type.\n",
    "\n",
    "Here is the code that replaces the \"?\" symbol with NaN in the \"bare_nuclei\" column and then converts the column's data type to float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after Data Type Conversion:  \n",
      "\n",
      "patient_id                 int64\n",
      "clump_thickness          float64\n",
      "cell_size_uniformity     float64\n",
      "cell_shape_uniformity      int64\n",
      "marginal_adhesion          int64\n",
      "single_ep_cell_size        int64\n",
      "bare_nuclei              float64\n",
      "bland_chromatin          float64\n",
      "normal_nucleoli          float64\n",
      "mitoses                    int64\n",
      "class                     object\n",
      "doctor_name               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd   \n",
    "import numpy as np\n",
    "\n",
    "# Load the breast cancer dataset \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "\n",
    "# Replace the '?' symbol with NaN  \n",
    "df['bare_nuclei'] = df['bare_nuclei'].replace('?', np.nan) \n",
    "\n",
    "# Convert columns to the appropriate data types \n",
    "df['bare_nuclei'] = df['bare_nuclei'].astype(float) \n",
    "\n",
    "# Print modified dataset \n",
    "print(\"Dataset after Data Type Conversion: \",\"\\n\") \n",
    "print(df.dtypes) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we replace all occurrences of '?' with NaN (Not a Number) using the replace() method from Pandas. After resolving this issue, we proceed to convert the 'bare_nuclei' column to a suitable data type. In this case, we convert it to a floating-point number data type using the astype() method, specifying float as the desired data type. This ensures that the 'bare_nuclei' column contains numeric values, enabling numerical calculations and analyses to be performed accurately.\n",
    "\n",
    "Finally, we print the modified dataset to verify the changes made. By examining the data types of all columns again using the dtypes attribute, we can confirm that the 'bare_nuclei' column now has the desired data type, which allows for consistent data processing and analysis.\n",
    "\n",
    "## Standardizing Categorical Values\n",
    "In the context of data cleaning, it is essential to standardize categorical values to ensure consistency and facilitate accurate analysis. One common scenario is when dealing with categorical columns containing values that may vary in their formatting or capitalization. By standardizing these values, we can eliminate inconsistencies and improve the overall quality of the dataset.\n",
    "\n",
    "In this case, the class values are \"benign\" and \"malignant.\" However, it is good practice to standardize the values by capitalizing the first letter, making it consistent throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     patient_id  clump_thickness  cell_size_uniformity  cell_shape_uniformity  \\\n",
      "0       1000025              5.0                   1.0                      1   \n",
      "1       1002945              5.0                   4.0                      4   \n",
      "2       1015425              3.0                   1.0                      1   \n",
      "3       1016277              6.0                   8.0                      8   \n",
      "4       1017023              4.0                   1.0                      1   \n",
      "..          ...              ...                   ...                    ...   \n",
      "694      776715              3.0                   1.0                      1   \n",
      "695      841769              2.0                   1.0                      1   \n",
      "696      888820              5.0                  10.0                     10   \n",
      "697      897471              4.0                   8.0                      6   \n",
      "698      897471              4.0                   8.0                      8   \n",
      "\n",
      "     marginal_adhesion  single_ep_cell_size bare_nuclei  bland_chromatin  \\\n",
      "0                    1                    2           1              3.0   \n",
      "1                    5                    7          10              3.0   \n",
      "2                    1                    2           2              3.0   \n",
      "3                    1                    3           4              3.0   \n",
      "4                    3                    2           1              3.0   \n",
      "..                 ...                  ...         ...              ...   \n",
      "694                  1                    3           2              1.0   \n",
      "695                  1                    2           1              1.0   \n",
      "696                  3                    7           3              8.0   \n",
      "697                  4                    3           4             10.0   \n",
      "698                  5                    4           5             10.0   \n",
      "\n",
      "     normal_nucleoli  mitoses      class doctor_name  \n",
      "0                1.0        1     benign     Dr. Doe  \n",
      "1                2.0        1     benign   Dr. Smith  \n",
      "2                1.0        1     benign     Dr. Lee  \n",
      "3                7.0        1     benign   Dr. Smith  \n",
      "4                1.0        1     benign    Dr. Wong  \n",
      "..               ...      ...        ...         ...  \n",
      "694              1.0        1     benign     Dr. Lee  \n",
      "695              1.0        1     benign   Dr. Smith  \n",
      "696             10.0        2  malignant     Dr. Lee  \n",
      "697              6.0        1  malignant     Dr. Lee  \n",
      "698              4.0        1  malignant    Dr. Wong  \n",
      "\n",
      "[699 rows x 12 columns]\n",
      "Dataset after Standardizing Class Values: \n",
      "0         Benign\n",
      "1         Benign\n",
      "2         Benign\n",
      "3         Benign\n",
      "4         Benign\n",
      "         ...    \n",
      "694       Benign\n",
      "695       Benign\n",
      "696    Malignant\n",
      "697    Malignant\n",
      "698    Malignant\n",
      "Name: class, Length: 699, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load the breast cancer dataset \n",
    "df = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/breast_cancer_data.csv\") \n",
    "print(df) \n",
    "\n",
    "# Standardize class values \n",
    "df['class'] = df['class'].str.capitalize() \n",
    "\n",
    "# Print modified dataset \n",
    "print(\"Dataset after Standardizing Class Values: \") \n",
    "print(df['class']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the \"class\" column is accessed using the column name 'class'. The str.capitalize() function is applied to each value in the 'class' column, which capitalizes the first letter of each value. This operation standardizes the class values by ensuring consistent capitalization.\n",
    "\n",
    "Finally, the modified column is printed to demonstrate the changes made."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
