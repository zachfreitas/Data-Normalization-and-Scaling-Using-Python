{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Data transformation is a process of modifying and converting raw data into a clean, structured, and organized format that is ready for further analysis and modeling. This process can involve several different steps such as merging data from multiple sources, reshaping the data to make it consistent, and grouping data to help reveal patterns and insights. The end result is a dataset that is standardized, optimized, and well-suited for statistical modeling and other types of data analysis.\n",
    "\n",
    "The importance of data transformation in data wrangling lies in the fact that it ensures that the data is prepared in a format that can be easily consumed and analyzed. Raw data is often in a format that is not suitable for analysis and modeling. Data transformation helps to reshape the data into a format that is easier to work with and better suited for further analysis.\n",
    "\n",
    "Data transformation helps to merge and integrate data from different sources into a single, unified data set. This helps to overcome the limitations of working with isolated data sources and enables more comprehensive and accurate analysis. Additionally, data transformation enables aggregation of the data, which involves summarizing and reducing the data to a more manageable and interpretable size. This is particularly useful when working with large datasets.\n",
    "\n",
    "In this module, we will cover the following topics:\n",
    "\n",
    "I. Merging data: Processes for combining data from multiple sources into a single, unified dataset.\n",
    "II. Aggregating data: Methods for summarizing and grouping data based on specified criteria.\n",
    "III. Reshaping data: Techniques for restructuring and rearranging data into desired forms.\n",
    "\n",
    "# Learning Objectives\n",
    "In this module, the learners will:\n",
    "\n",
    "* Understand how to convert raw data into a clean and structured format\n",
    "* Synthesize information from multiple datasets to create a unified dataset\n",
    "* Summarize and interpret data to produce valuable insights\n",
    "* Evaluate the quality of data to facilitate effective decision-making\n",
    "Let's get started!\n",
    "\n",
    "# Dataset\n",
    "## Titanic dataset\n",
    "This is a well-known and widely used dataset in the field of data analysis and machine learning. This dataset contains information about the passengers on the Titanic ship, including their demographic information, ticket information, and survival status. In this exercise, we're using the Titanic dataset and its two sub-datasets (passengers_personal_info and travel_info) to demonstrate the merging, aggregating, and reshaping of data through Pandas functions.\n",
    "\n",
    "Here's a description of the columns in the dataset:\n",
    "\n",
    "* PassengerId: This column is a unique identifier assigned to each passenger.\n",
    "* Age: This column specifies the age of the passenger.\n",
    "* Name: This column specifies the name of the passenger.\n",
    "* Sex: This column specifies the gender of the passenger (Male or Female).\n",
    "* Survived: This column specifies whether the passenger survived the Titanic disaster or not. The values in this column can either be 0 (did not survive) or 1 (survived).\n",
    "* Pclass (Passenger Class): This column specifies the class of the passenger (1st, 2nd, or 3rd class).\n",
    "* SibSp (Siblings/Spouses Aboard): This column specifies the number of siblings or spouses the passenger was traveling with.\n",
    "* Parch (Parents/Children Aboard): This column specifies the number of parents or children the passenger was traveling with.\n",
    "* Ticket: This column specifies the ticket number assigned to the passenger.\n",
    "* Fare: This column specifies the fare paid by the passenger for their ticket.\n",
    "* Cabin: This column specifies the cabin number assigned to the passenger.\n",
    "* Embarked: This column specifies the port where the passenger boarded the Titanic (C = Cherbourg; Q = Queenstown; S = Southampton).\n",
    "\n",
    "## Loading datasets\n",
    "This code imports the Pandas library and reads a CSV file that contains the personal information of Titanic passengers. The dataset includes information about the passenger id, name, age, and sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId   Age                                               Name  \\\n",
      "0            1  22.0                            Braund, Mr. Owen Harris   \n",
      "1            2  38.0  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2            3  26.0                             Heikkinen, Miss. Laina   \n",
      "3            4  35.0       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "4            6   NaN                                   Moran, Mr. James   \n",
      "5            7  54.0                            McCarthy, Mr. Timothy J   \n",
      "6            8   2.0                     Palsson, Master. Gosta Leonard   \n",
      "7           10  14.0                Nasser, Mrs. Nicholas (Adele Achem)   \n",
      "8           11   4.0                    Sandstrom, Miss. Marguerite Rut   \n",
      "\n",
      "      Sex  \n",
      "0    male  \n",
      "1  female  \n",
      "2  female  \n",
      "3  female  \n",
      "4    male  \n",
      "5    male  \n",
      "6    male  \n",
      "7  female  \n",
      "8  female  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Use the 'pd.read_csv' method to read the \"passengers_personal_info.csv\" file\n",
    "passengers_personal_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/passengers_personal_info.csv\")\n",
    "print(passengers_personal_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reads a CSV file named \"travel_info.csv\", which contains the travel information of Titanic passengers. This dataset includes Survived, Pclass, SibSp, Parch, Ticket, Fare, Cabin, and Embarked columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  SibSp  Parch            Ticket     Fare  \\\n",
      "0            1         0       3      1      0         A/5 21171   7.2500   \n",
      "1            2         1       1      1      0          PC 17599  71.2833   \n",
      "2            3         1       3      0      0  STON/O2. 3101282   7.9250   \n",
      "3            5         0       3      0      0            373450   8.0500   \n",
      "4            6         0       3      0      0            330877   8.4583   \n",
      "5            7         0       1      0      0             17463  51.8625   \n",
      "6            8         0       3      3      1            349909  21.0750   \n",
      "7            9         1       3      0      2            347742  11.1333   \n",
      "8           11         1       3      1      1           PP 9549  16.7000   \n",
      "\n",
      "  Cabin Embarked  \n",
      "0   NaN        S  \n",
      "1   C85        C  \n",
      "2   NaN        S  \n",
      "3   NaN        S  \n",
      "4   NaN        Q  \n",
      "5   E46        S  \n",
      "6   NaN        S  \n",
      "7   NaN        S  \n",
      "8    G6        S  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Use the 'pd.read_csv' method to read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\")\n",
    "print(travel_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is merging data?\n",
    "The process of merging data is a common task in data analysis and data science, as it allows you to combine data from multiple sources into a single data structure for analysis and modeling. This process can be performed in different ways, depending on the type of join operation to be performed.\n",
    "\n",
    "## Why is it important?\n",
    "One common use case is analyzing customer behavior for businesses. By merging customer transaction and demographic data, businesses can gain insights into their customers' behavior, preferences, and needs to inform marketing strategies, product development, and customer service initiatives.\n",
    "\n",
    "In summary, merging data is a crucial step in many data analysis tasks. By combining data from multiple sources, businesses can gain deeper insights into their customers and make data-driven decisions to improve their overall performance.\n",
    "\n",
    "## Performing inner join\n",
    "An inner join only includes rows that have matching values in both datasets. To perform an inner join, we use the 'merge()' function which combines two dataframes into a single one, based on a common column like 'PassengerId'.\n",
    "\n",
    "The resulting merged dataset contains only the rows where there is a match, effectively creating a subset of the original datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId   Age                                               Name  \\\n",
      "0            1  22.0                            Braund, Mr. Owen Harris   \n",
      "1            2  38.0  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2            3  26.0                             Heikkinen, Miss. Laina   \n",
      "3            6   NaN                                   Moran, Mr. James   \n",
      "4            7  54.0                            McCarthy, Mr. Timothy J   \n",
      "5            8   2.0                     Palsson, Master. Gosta Leonard   \n",
      "6           11   4.0                    Sandstrom, Miss. Marguerite Rut   \n",
      "\n",
      "      Sex  Survived  Pclass  SibSp  Parch            Ticket     Fare Cabin  \\\n",
      "0    male         0       3      1      0         A/5 21171   7.2500   NaN   \n",
      "1  female         1       1      1      0          PC 17599  71.2833   C85   \n",
      "2  female         1       3      0      0  STON/O2. 3101282   7.9250   NaN   \n",
      "3    male         0       3      0      0            330877   8.4583   NaN   \n",
      "4    male         0       1      0      0             17463  51.8625   E46   \n",
      "5    male         0       3      3      1            349909  21.0750   NaN   \n",
      "6  female         1       3      1      1           PP 9549  16.7000    G6   \n",
      "\n",
      "  Embarked  \n",
      "0        S  \n",
      "1        C  \n",
      "2        S  \n",
      "3        Q  \n",
      "4        S  \n",
      "5        S  \n",
      "6        S  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd \n",
    "\n",
    "# # Read the \"passengers_personal_info.csv\" file\n",
    "passengers_personal_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/passengers_personal_info.csv\")\n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\")\n",
    "\n",
    "# Perform an inner join on the 'PassengerId' column\n",
    "# Only the rows that have matching values in both datasets will be included in the result\n",
    "inner_join = pd.merge(passengers_personal_info, travel_info, on='PassengerId', how='inner')\n",
    "print(inner_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above result of the inner join, certain cells have been skipped. This is because some of the values for the 'PassengerId' column are present in the left dataset ('passengers_personal_info') but not in the right dataset ('travel_info'), and vice versa.\n",
    "\n",
    "## Performing left join\n",
    "A left join returns all the rows from the left dataset (in this case, the  'passengers_personal_info' dataset) and only the matching rows from the right dataset (in this case, the 'ticket_info' dataset). If there are no matching rows in the right dataset, the result will have NaN values for the columns from the right dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId   Age                                               Name  \\\n",
      "0            1  22.0                            Braund, Mr. Owen Harris   \n",
      "1            2  38.0  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2            3  26.0                             Heikkinen, Miss. Laina   \n",
      "3            4  35.0       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "4            6   NaN                                   Moran, Mr. James   \n",
      "5            7  54.0                            McCarthy, Mr. Timothy J   \n",
      "6            8   2.0                     Palsson, Master. Gosta Leonard   \n",
      "7           10  14.0                Nasser, Mrs. Nicholas (Adele Achem)   \n",
      "8           11   4.0                    Sandstrom, Miss. Marguerite Rut   \n",
      "\n",
      "      Sex  Survived  Pclass  SibSp  Parch            Ticket     Fare Cabin  \\\n",
      "0    male       0.0     3.0    1.0    0.0         A/5 21171   7.2500   NaN   \n",
      "1  female       1.0     1.0    1.0    0.0          PC 17599  71.2833   C85   \n",
      "2  female       1.0     3.0    0.0    0.0  STON/O2. 3101282   7.9250   NaN   \n",
      "3  female       NaN     NaN    NaN    NaN               NaN      NaN   NaN   \n",
      "4    male       0.0     3.0    0.0    0.0            330877   8.4583   NaN   \n",
      "5    male       0.0     1.0    0.0    0.0             17463  51.8625   E46   \n",
      "6    male       0.0     3.0    3.0    1.0            349909  21.0750   NaN   \n",
      "7  female       NaN     NaN    NaN    NaN               NaN      NaN   NaN   \n",
      "8  female       1.0     3.0    1.0    1.0           PP 9549  16.7000    G6   \n",
      "\n",
      "  Embarked  \n",
      "0        S  \n",
      "1        C  \n",
      "2        S  \n",
      "3      NaN  \n",
      "4        Q  \n",
      "5        S  \n",
      "6        S  \n",
      "7      NaN  \n",
      "8        S  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the \"passengers_personal_info.csv\" file\n",
    "passengers_personal_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/passengers_personal_info.csv\")\n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\")\n",
    "\n",
    "# Perform a left join on the 'PassengerId' column\n",
    "left_join = pd.merge(passengers_personal_info, travel_info, on='PassengerId', how='left')\n",
    "print(left_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above performs a left join using the 'PassengerId' column as the join key. All rows from the 'passengers_personal_info' dataset will be included in the result, and any missing values from the 'travel_info' dataset will be filled with NaN.\n",
    "\n",
    "## Performing right join\n",
    "Similarly, a right join returns all the rows from the right dataset (in this case, the 'ticket_info' dataset) and the matching rows from the left dataset (in this case, the 'passengers_personal_info'). If there are no matching rows in the left dataset, the result will have NaN values for the columns from the left dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId   Age                                               Name  \\\n",
      "0            1  22.0                            Braund, Mr. Owen Harris   \n",
      "1            2  38.0  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2            3  26.0                             Heikkinen, Miss. Laina   \n",
      "3            5   NaN                                                NaN   \n",
      "4            6   NaN                                   Moran, Mr. James   \n",
      "5            7  54.0                            McCarthy, Mr. Timothy J   \n",
      "6            8   2.0                     Palsson, Master. Gosta Leonard   \n",
      "7            9   NaN                                                NaN   \n",
      "8           11   4.0                    Sandstrom, Miss. Marguerite Rut   \n",
      "\n",
      "      Sex  Survived  Pclass  SibSp  Parch            Ticket     Fare Cabin  \\\n",
      "0    male         0       3      1      0         A/5 21171   7.2500   NaN   \n",
      "1  female         1       1      1      0          PC 17599  71.2833   C85   \n",
      "2  female         1       3      0      0  STON/O2. 3101282   7.9250   NaN   \n",
      "3     NaN         0       3      0      0            373450   8.0500   NaN   \n",
      "4    male         0       3      0      0            330877   8.4583   NaN   \n",
      "5    male         0       1      0      0             17463  51.8625   E46   \n",
      "6    male         0       3      3      1            349909  21.0750   NaN   \n",
      "7     NaN         1       3      0      2            347742  11.1333   NaN   \n",
      "8  female         1       3      1      1           PP 9549  16.7000    G6   \n",
      "\n",
      "  Embarked  \n",
      "0        S  \n",
      "1        C  \n",
      "2        S  \n",
      "3        S  \n",
      "4        Q  \n",
      "5        S  \n",
      "6        S  \n",
      "7        S  \n",
      "8        S  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the \"passengers_personal_info.csv\" file\n",
    "passengers_personal_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/passengers_personal_info.csv\")\n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\")\n",
    "\n",
    "# Perform a right join on the 'PassengerId' column\n",
    "right_join = pd.merge(passengers_personal_info, travel_info, on='PassengerId', how='right')\n",
    "print(right_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above performs a right join using the 'PassengerId' column as the join key. All rows from the 'travel_info' dataset will be included in the result, and any missing values from the 'passengers_personal_info' dataset will be filled with NaN.\n",
    "\n",
    "## Performing outer join\n",
    "An outer join returns all the rows from both datasets, with matching rows combined and non-matching rows filled with NaN values for the columns that do not have matching data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PassengerId   Age                                               Name  \\\n",
      "0             1  22.0                            Braund, Mr. Owen Harris   \n",
      "1             2  38.0  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2             3  26.0                             Heikkinen, Miss. Laina   \n",
      "3             4  35.0       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "4             6   NaN                                   Moran, Mr. James   \n",
      "5             7  54.0                            McCarthy, Mr. Timothy J   \n",
      "6             8   2.0                     Palsson, Master. Gosta Leonard   \n",
      "7            10  14.0                Nasser, Mrs. Nicholas (Adele Achem)   \n",
      "8            11   4.0                    Sandstrom, Miss. Marguerite Rut   \n",
      "9             5   NaN                                                NaN   \n",
      "10            9   NaN                                                NaN   \n",
      "\n",
      "       Sex  Survived  Pclass  SibSp  Parch            Ticket     Fare Cabin  \\\n",
      "0     male       0.0     3.0    1.0    0.0         A/5 21171   7.2500   NaN   \n",
      "1   female       1.0     1.0    1.0    0.0          PC 17599  71.2833   C85   \n",
      "2   female       1.0     3.0    0.0    0.0  STON/O2. 3101282   7.9250   NaN   \n",
      "3   female       NaN     NaN    NaN    NaN               NaN      NaN   NaN   \n",
      "4     male       0.0     3.0    0.0    0.0            330877   8.4583   NaN   \n",
      "5     male       0.0     1.0    0.0    0.0             17463  51.8625   E46   \n",
      "6     male       0.0     3.0    3.0    1.0            349909  21.0750   NaN   \n",
      "7   female       NaN     NaN    NaN    NaN               NaN      NaN   NaN   \n",
      "8   female       1.0     3.0    1.0    1.0           PP 9549  16.7000    G6   \n",
      "9      NaN       0.0     3.0    0.0    0.0            373450   8.0500   NaN   \n",
      "10     NaN       1.0     3.0    0.0    2.0            347742  11.1333   NaN   \n",
      "\n",
      "   Embarked  \n",
      "0         S  \n",
      "1         C  \n",
      "2         S  \n",
      "3       NaN  \n",
      "4         Q  \n",
      "5         S  \n",
      "6         S  \n",
      "7       NaN  \n",
      "8         S  \n",
      "9         S  \n",
      "10        S  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the \"passengers_personal_info.csv\" file\n",
    "passengers_personal_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/passengers_personal_info.csv\")\n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\") \n",
    "\n",
    "# Perform an outer join on the 'PassengerId' column\n",
    "\n",
    "# All rows from both datasets will be included in the result, and any \n",
    "\n",
    "# missing values will be filled with NaN\n",
    "outer_join = pd.merge(passengers_personal_info, travel_info, on='PassengerId', how='outer')\n",
    "print(outer_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the result above, the order of the rows with null values can change depending on the order of the datasets in the join. You can use the 'sort' parameter in the Pandas 'merge()' function to specify the sorting behavior of the merged result based on the join key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PassengerId   Age                                               Name  \\\n",
      "0             1  22.0                            Braund, Mr. Owen Harris   \n",
      "1             2  38.0  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2             3  26.0                             Heikkinen, Miss. Laina   \n",
      "3             4  35.0       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "4             5   NaN                                                NaN   \n",
      "5             6   NaN                                   Moran, Mr. James   \n",
      "6             7  54.0                            McCarthy, Mr. Timothy J   \n",
      "7             8   2.0                     Palsson, Master. Gosta Leonard   \n",
      "8             9   NaN                                                NaN   \n",
      "9            10  14.0                Nasser, Mrs. Nicholas (Adele Achem)   \n",
      "10           11   4.0                    Sandstrom, Miss. Marguerite Rut   \n",
      "\n",
      "       Sex  Survived  Pclass  SibSp  Parch            Ticket     Fare Cabin  \\\n",
      "0     male       0.0     3.0    1.0    0.0         A/5 21171   7.2500   NaN   \n",
      "1   female       1.0     1.0    1.0    0.0          PC 17599  71.2833   C85   \n",
      "2   female       1.0     3.0    0.0    0.0  STON/O2. 3101282   7.9250   NaN   \n",
      "3   female       NaN     NaN    NaN    NaN               NaN      NaN   NaN   \n",
      "4      NaN       0.0     3.0    0.0    0.0            373450   8.0500   NaN   \n",
      "5     male       0.0     3.0    0.0    0.0            330877   8.4583   NaN   \n",
      "6     male       0.0     1.0    0.0    0.0             17463  51.8625   E46   \n",
      "7     male       0.0     3.0    3.0    1.0            349909  21.0750   NaN   \n",
      "8      NaN       1.0     3.0    0.0    2.0            347742  11.1333   NaN   \n",
      "9   female       NaN     NaN    NaN    NaN               NaN      NaN   NaN   \n",
      "10  female       1.0     3.0    1.0    1.0           PP 9549  16.7000    G6   \n",
      "\n",
      "   Embarked  \n",
      "0         S  \n",
      "1         C  \n",
      "2         S  \n",
      "3       NaN  \n",
      "4         S  \n",
      "5         Q  \n",
      "6         S  \n",
      "7         S  \n",
      "8         S  \n",
      "9       NaN  \n",
      "10        S  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the \"passengers_personal_info.csv\" file\n",
    "passengers_personal_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/passengers_personal_info.csv\")\n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\") \n",
    "\n",
    "# Perform an outer join on the 'PassengerId' column\n",
    "\n",
    "# All rows from both datasets will be included in the result, and any \n",
    "\n",
    "# Perform an outer join on the 'PassengerId' column with sort parameter\n",
    "outer_join = pd.merge(passengers_personal_info, travel_info, on='PassengerId', how='outer', sort=True)\n",
    "print(outer_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "In Pandas, you can perform join operations using either the 'join()' function or the 'merge()' function. While 'join()' is a convenient wrapper for common join operations, 'merge()' offers more flexibility and control. It's recommended to use 'merge()' for most join operations, and 'join()' only when joining on indices instead of columns.\n",
    "\n",
    "\n",
    "## Performing cross join\n",
    "A cross-join, also known as a cartesian product, is a join operation. It returns a result set that includes every possible combination of rows from both tables, where each row from the first table is paired with each row from the second table.\n",
    "\n",
    "Here's an example of performing a cross-join between 'passengers_personal_info' and 'travel_info' datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PassengerId_x   Age                             Name     Sex  \\\n",
      "0               1  22.0          Braund, Mr. Owen Harris    male   \n",
      "1               1  22.0          Braund, Mr. Owen Harris    male   \n",
      "2               1  22.0          Braund, Mr. Owen Harris    male   \n",
      "3               1  22.0          Braund, Mr. Owen Harris    male   \n",
      "4               1  22.0          Braund, Mr. Owen Harris    male   \n",
      "..            ...   ...                              ...     ...   \n",
      "76             11   4.0  Sandstrom, Miss. Marguerite Rut  female   \n",
      "77             11   4.0  Sandstrom, Miss. Marguerite Rut  female   \n",
      "78             11   4.0  Sandstrom, Miss. Marguerite Rut  female   \n",
      "79             11   4.0  Sandstrom, Miss. Marguerite Rut  female   \n",
      "80             11   4.0  Sandstrom, Miss. Marguerite Rut  female   \n",
      "\n",
      "    PassengerId_y  Survived  Pclass  SibSp  Parch            Ticket     Fare  \\\n",
      "0               1         0       3      1      0         A/5 21171   7.2500   \n",
      "1               2         1       1      1      0          PC 17599  71.2833   \n",
      "2               3         1       3      0      0  STON/O2. 3101282   7.9250   \n",
      "3               5         0       3      0      0            373450   8.0500   \n",
      "4               6         0       3      0      0            330877   8.4583   \n",
      "..            ...       ...     ...    ...    ...               ...      ...   \n",
      "76              6         0       3      0      0            330877   8.4583   \n",
      "77              7         0       1      0      0             17463  51.8625   \n",
      "78              8         0       3      3      1            349909  21.0750   \n",
      "79              9         1       3      0      2            347742  11.1333   \n",
      "80             11         1       3      1      1           PP 9549  16.7000   \n",
      "\n",
      "   Cabin Embarked  \n",
      "0    NaN        S  \n",
      "1    C85        C  \n",
      "2    NaN        S  \n",
      "3    NaN        S  \n",
      "4    NaN        Q  \n",
      "..   ...      ...  \n",
      "76   NaN        Q  \n",
      "77   E46        S  \n",
      "78   NaN        S  \n",
      "79   NaN        S  \n",
      "80    G6        S  \n",
      "\n",
      "[81 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd \n",
    "\n",
    "# Read the \"passengers_personal_info.csv\" file\n",
    "passengers_personal_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/passengers_personal_info.csv\")\n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\")\n",
    "\n",
    "# The 'assign' method is used to add a new column \"key\" to both DataFrames.\n",
    "# This is necessary because the merge method requires a common key to perform \n",
    "# the join operation. Although both DataFrames have a column called \n",
    "# PassengerId, this column cannot be used as the common key because the \n",
    "# values are not unique across the two DataFrames. \n",
    "\n",
    "# In this case, the column is assigned a constant value of 1 for all rows.\n",
    "\n",
    "# Finally, the \"key\" column is dropped from the resulting DataFrame using the drop method. \n",
    "# This is done because the column was only added as a means to perform the join, and \n",
    "# it is no longer needed in the final DataFrame.\n",
    "cross_join = passengers_personal_info.assign(key=1).merge(travel_info.assign(key=1), on='key', how='outer').drop('key', axis=1)\n",
    "print(cross_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame contains all possible combinations of rows from 'passengers_personal_info' and 'travel_info'. Note that if the two input DataFrames have 'n' rows each, the resulting DataFrame will have 'n' rows x 'n' rows = 'n^2' rows.\n",
    "\n",
    "\n",
    "## Performing concatenation\n",
    "Concatenation is the process of combining multiple datasets into a single dataset. In the case of the Titanic dataset, concatenation can be useful when you have separate datasets for each class or each embarkation port and you want to combine them into a single dataset for analysis.\n",
    "\n",
    "Let's see the result when we combine the 'passengers_personal_info' and 'travel_info' datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId   Age                                               Name  \\\n",
      "0            1  22.0                            Braund, Mr. Owen Harris   \n",
      "1            2  38.0  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "2            3  26.0                             Heikkinen, Miss. Laina   \n",
      "3            4  35.0       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
      "4            6   NaN                                   Moran, Mr. James   \n",
      "5            7  54.0                            McCarthy, Mr. Timothy J   \n",
      "6            8   2.0                     Palsson, Master. Gosta Leonard   \n",
      "7           10  14.0                Nasser, Mrs. Nicholas (Adele Achem)   \n",
      "8           11   4.0                    Sandstrom, Miss. Marguerite Rut   \n",
      "\n",
      "      Sex  PassengerId  Survived  Pclass  SibSp  Parch            Ticket  \\\n",
      "0    male            1         0       3      1      0         A/5 21171   \n",
      "1  female            2         1       1      1      0          PC 17599   \n",
      "2  female            3         1       3      0      0  STON/O2. 3101282   \n",
      "3  female            5         0       3      0      0            373450   \n",
      "4    male            6         0       3      0      0            330877   \n",
      "5    male            7         0       1      0      0             17463   \n",
      "6    male            8         0       3      3      1            349909   \n",
      "7  female            9         1       3      0      2            347742   \n",
      "8  female           11         1       3      1      1           PP 9549   \n",
      "\n",
      "      Fare Cabin Embarked  \n",
      "0   7.2500   NaN        S  \n",
      "1  71.2833   C85        C  \n",
      "2   7.9250   NaN        S  \n",
      "3   8.0500   NaN        S  \n",
      "4   8.4583   NaN        Q  \n",
      "5  51.8625   E46        S  \n",
      "6  21.0750   NaN        S  \n",
      "7  11.1333   NaN        S  \n",
      "8  16.7000    G6        S  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the \"passengers_personal_info.csv\" file\n",
    "passengers_personal_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/passengers_personal_info.csv\")\n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\")\n",
    "\n",
    "# Concatenate the two datasets\n",
    "# The 'axis' parameter determines the axis along which to concatenate the dataframes\n",
    "titanic_concat = pd.concat([passengers_personal_info, travel_info], axis=1)\n",
    "\n",
    "# Preview the resulting dataset\n",
    "print(titanic_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, 'axis = 1' refers to concatenating dataframes horizontally, along the columns. This means the columns of one dataframe are added to the columns of the other dataframe.\n",
    "\n",
    "When 'axis = 0' is used in the 'pd.concat()' function, the dataframes are concatenated vertically. The result is the rows of one dataframe followed by the rows of the other dataframe.\n",
    "\n",
    "### Note \n",
    "It is worth noting that concatenation can be performed on any set of columns in the Titanic dataset, not just the embarkation port and class columns. The choice of which columns to concatenate depends on the hypothesis being explored and the availability of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating Data\n",
    "\n",
    "## What is aggregating data?\n",
    "Aggregation is an essential part of data transformation, as it helps simplify and summarize complex data, making it easier to understand and analyze. This can be done through techniques such as grouping, summarizing, and calculating statistical measures, which provide a clearer picture of the data being analyzed.\n",
    "\n",
    "## Why is it important?\n",
    "Without aggregation, analyzing large and complex data sets would be a challenging and time-consuming task. By simplifying and summarizing data, aggregation enables analysts and data scientists to identify patterns, trends, and insights that may not be apparent from examining individual data points.\n",
    "\n",
    "One use case for aggregation is in analyzing website traffic data. Websites typically generate a large amount of data, including user behavior, page views, and referral sources. By aggregating website traffic data, analysts can identify peak traffic times, popular pages, and referral sources that generate the most traffic. This information can be used to optimize website design and content, improve user experience, and increase engagement and conversions.\n",
    "\n",
    "## Grouping data\n",
    "Grouping is a technique in data transformation used to aggregate and summarize data based on a specific attribute or feature. We will apply grouping in the 'travel_info' dataset to explore relationships within the data and make it easier to analyze and understand.\n",
    "\n",
    "Here is an example of how you can use grouping on the 'travel_info' dataset to count the number of passengers who survived:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived\n",
      "0    5\n",
      "1    4\n",
      "Name: PassengerId, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\")\n",
    "\n",
    "# The groupby function is used to group rows of a DataFrame based on the values in 'Survived' column\n",
    "grouped = travel_info.groupby('Survived')\n",
    "\n",
    "# Count the number of passengers who survived\n",
    "survived = grouped['PassengerId'].count()\n",
    "print(survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting survived dataframe will contain the number of passengers who survived (value of 1) and those who did not survive (value of 0).\n",
    "\n",
    "### NOTE\n",
    "In the provided code, the 'count()' function is applied to the 'PassengerId' column to count the number of passengers who survived in each group. However, other aggregating functions such as mean, sum, median, min, or max can also be used based on the hypothesis being investigated. It is essential to choose the appropriate aggregating function and ensure that the data being analyzed is appropriate for the chosen function.\n",
    "\n",
    "\n",
    "## Binning data\n",
    "Binning is a technique to group continuous or large-scale data into smaller, more manageable, meaningful intervals or \"bins\". This process helps reduce the noise and complexity of the data, making it easier to visualize and understand.\n",
    "\n",
    "The following code creates a new column 'Fare_Range' in the 'travel_info' dataframe by dividing the 'Fare' column into ranges of 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  SibSp  Parch            Ticket     Fare  \\\n",
      "0            1         0       3      1      0         A/5 21171   7.2500   \n",
      "1            2         1       1      1      0          PC 17599  71.2833   \n",
      "2            3         1       3      0      0  STON/O2. 3101282   7.9250   \n",
      "3            5         0       3      0      0            373450   8.0500   \n",
      "4            6         0       3      0      0            330877   8.4583   \n",
      "5            7         0       1      0      0             17463  51.8625   \n",
      "6            8         0       3      3      1            349909  21.0750   \n",
      "7            9         1       3      0      2            347742  11.1333   \n",
      "8           11         1       3      1      1           PP 9549  16.7000   \n",
      "\n",
      "  Cabin Embarked Fare_Range  \n",
      "0   NaN        S       0-10  \n",
      "1   C85        C      70-80  \n",
      "2   NaN        S       0-10  \n",
      "3   NaN        S       0-10  \n",
      "4   NaN        Q       0-10  \n",
      "5   E46        S      50-60  \n",
      "6   NaN        S      20-30  \n",
      "7   NaN        S      10-20  \n",
      "8    G6        S      10-20  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd \n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\")\n",
    "\n",
    "# Create a new column 'Fare_Range' by applying binning on the 'Fare' column\n",
    "travel_info['Fare_Range'] = pd.cut(travel_info['Fare'], bins=[0, 10, 20, 30, 40, 50, 60, 70, travel_info['Fare'].max()], labels=['0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80'])\n",
    "\n",
    "# Verify the result\n",
    "print(travel_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the 'pd. cut()' function is used to apply binning on the 'Fare' column and create a new column 'Fare_Range'. The 'bins' parameter is used to specify the bin ranges and the 'labels' parameter is used to specify the labels for each bin range. The resulting 'Fare_Range' column will contain the binned values for each passenger's fare.\n",
    "\n",
    "## Performing statistical measures\n",
    "Statistical measures are crucial in data aggregation as they simplify the process of summarizing and comprehending large and complex datasets. For instance, the mean, median, and standard deviation of the 'Age' column in the 'passengers_personal_info' dataset can provide insights into the average age of the passengers, and the spread of these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean age: 24.375\n",
      "Median age: 24.0\n",
      "Standard deviation of age: 17.75980614437315\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd  \n",
    "\n",
    "# Read the \"passengers_personal_info.csv\" file\n",
    "passengers_personal_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/passengers_personal_info.csv\")\n",
    "\n",
    "# Calculate the mean of the \"Age\" column\n",
    "mean_age = passengers_personal_info[\"Age\"].mean()\n",
    "print(\"Mean age:\", mean_age)\n",
    "\n",
    "# Calculate the median of the \"Age\" column\n",
    "median_age = passengers_personal_info[\"Age\"].median()\n",
    "print(\"Median age:\", median_age)\n",
    "\n",
    "# Calculate the standard deviation of the \"Age\" column\n",
    "std_age = passengers_personal_info[\"Age\"].std()\n",
    "print(\"Standard deviation of age:\", std_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset shows that the passengers on the Titanic had a mean age of 24.375 years, indicating that they were a relatively young population. The median age of 24 years suggests that half of the passengers were 24 years old or younger, while the other half were older than 24. The standard deviation of age was 17.759, indicating that the age distribution was quite spread out.\n",
    "\n",
    "The 'Fare' column in the 'travel_info' dataset can also be analyzed using mean, median, and standard deviation to understand the average fare and the spread of fares of passengers. This information helps determine the typical fare range and draw conclusions about fare distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean fare: 22.637488888888885\n",
      "Median fare: 11.1333\n",
      "Standard deviation of fare: 23.069552699849016\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\")\n",
    "\n",
    "# Calculate the mean of the \"Fare\" column\n",
    "mean_fare = travel_info[\"Fare\"].mean()\n",
    "print(\"Mean fare:\", mean_fare)\n",
    "\n",
    "# Calculate the median of the \"Fare\" column\n",
    "median_fare = travel_info[\"Fare\"].median()\n",
    "print(\"Median fare:\", median_fare)\n",
    "\n",
    "# Calculate the standard deviation of the \"Fare\" column\n",
    "std_fare = travel_info[\"Fare\"].std()\n",
    "print(\"Standard deviation of fare:\", std_fare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset shows that the mean fare paid by passengers on the Titanic was 22.64 dollars, indicating that the average fare was relatively low. The median fare of 11.13 dollars suggests that half of the passengers paid less than this amount for their ticket, while the other half paid more. The standard deviation of fare was 23.07 dollars, indicating that there was a wide range of fares paid by passengers, with some paying significantly more or less than the average fare. This variability in fares may reflect differences in ticket class or other factors.\n",
    "\n",
    "## Performing multi-level indexing\n",
    "The technique of multilevel indexing is important for aggregating data across multiple dimensions. This means that we can analyze data based on different criteria at the same time. In the Titanic dataset, we use multilevel indexing to group data by both the 'Pclass' and 'Embarked' columns. This allows us to investigate the relationship between these two dimensions and other columns such as 'Age' and 'Fare'. By using multilevel indexing, we can easily compare and contrast different subgroups of the data and see how they relate to each other. \n",
    "\n",
    "This can provide valuable insights into patterns and trends in the data that may not be apparent when looking at the data as a whole. Ultimately, multilevel indexing is a powerful tool for exploring complex datasets and uncovering hidden relationships between different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Age                          Fare                    \n",
      "                      mean median        std        mean   median        std\n",
      "Pclass Embarked                                                             \n",
      "1      C         38.027027   36.5  14.243454  104.718529  78.2667  99.093935\n",
      "       Q         38.500000   38.5   7.778175   90.000000  90.0000   0.000000\n",
      "       S         38.152037   37.0  15.315584   70.364862  52.0000  58.811278\n",
      "2      C         22.766667   25.0  10.192551   25.358335  24.0000  11.345067\n",
      "       Q         43.500000   43.5  19.091883   12.350000  12.3500   0.000000\n",
      "       S         30.386731   30.0  14.080001   20.327439  13.5000  13.630741\n",
      "3      C         20.741951   20.0  11.712367   11.214083   7.8958   4.871528\n",
      "       Q         25.937500   21.5  16.807938   11.183393   7.7500   6.721677\n",
      "       S         25.696552   25.0  12.110906   14.644083   8.0500  13.276609\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = pd.read_csv('https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/titanic.csv')\n",
    "\n",
    "# Create a multi-level index based on \"Pclass\" and \"Embarked\" columns\n",
    "grouped = titanic.groupby(['Pclass', 'Embarked'])\n",
    "\n",
    "# Calculate the mean, median, and standard deviation of \"Age\" and \"Fare\" columns\n",
    "result = grouped.agg({'Age': ['mean', 'median', 'std'], 'Fare': ['mean', 'median', 'std']})\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the 'agg()' method takes a dictionary of columns and aggregation functions, where the keys are the columns to aggregate and the values are the aggregation functions to apply.\n",
    "\n",
    "The output shows the mean, median, and standard deviation of the 'Age' and 'Fare' columns for each combination of 'Pclass' and 'Embarked'. It can be observed that for 'Pclass' 1, passengers who embarked from Cherbourg (C) paid the highest mean fare of 104.72, whereas those who embarked from Queenstown (Q) paid the highest mean fare of 90.00. Passengers who embarked from Southampton (S) paid the lowest mean fare for the 'Pclass' 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping Data\n",
    "\n",
    "## What is reshaping data?\n",
    "Reshaping data is the process of transforming data from one layout or structure to another. This is a common task in data analysis and involves rearranging data into a more convenient format for analysis, visualization, or storage.\n",
    "\n",
    "## Why is it important?\n",
    "Reshaping data is important in data transformation as it allows to change the structure and format of the data into a more suitable or readable form. This can improve the data's overall accessibility, make it easier to perform subsequent analysis or visualization, and help in data-driven decision-making.\n",
    "\n",
    "One use case for reshaping data is in analyzing customer feedback data. Customer feedback data can come in different formats, such as text, audio, or video, and from various sources, such as social media, email, or surveys. Reshaping this data can help extract valuable insights and trends that may not be apparent from analyzing the raw data.\n",
    "\n",
    "For example, sentiment analysis is a common technique used to analyze customer feedback data. It involves categorizing customer feedback into positive, negative, or neutral sentiments. Reshaping the data into a structured format, such as a table with sentiment categories and corresponding frequencies, can make it easier to perform sentiment analysis and identify the overall sentiment trends. This information can be used to improve customer satisfaction, identify areas for improvement, and inform marketing strategies.\n",
    "\n",
    "## Creating pivot tables\n",
    "A pivot table in Pandas is a powerful tool that allows users to reshape and summarize data in a compact format. With the 'pivot_table()' method, data is aggregated and grouped by one or more variables to create a new, summarized dataframe.\n",
    "\n",
    "We will pivot the Titanic dataset using the 'Pclass' and 'Embarked' columns as the index, and the 'Fare' column as the values. We chose these columns as the index because they are categorical variables that define different groups of passengers based on their class and port of embarkation, and we want to see how these groups differ in terms of their fare. By using 'Fare' as the value, we can see the average fare for each group of passengers, which allows us to compare the fare across different classes and ports of embarkation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Fare\n",
      "Pclass Embarked            \n",
      "1      C         104.718529\n",
      "       Q          90.000000\n",
      "       S          70.364862\n",
      "2      C          25.358335\n",
      "       Q          12.350000\n",
      "       S          20.327439\n",
      "3      C          11.214083\n",
      "       Q          11.183393\n",
      "       S          14.644083\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = pd.read_csv('https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/titanic.csv')\n",
    "\n",
    "# Pivot the data using the 'Pclass' and 'Embarked' columns as the index, and the 'Fare' column as the values\n",
    "pivot = titanic.pivot_table(index=['Pclass', 'Embarked'], values='Fare')\n",
    "\n",
    "# Show the result\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the index is based on the 'Pclass' and 'Embarked' columns and the values are based on the 'Fare' column. The resulting pivot table shows the average fare for each combination of 'Pclass' and 'Embarked' values.\n",
    "\n",
    "Now, we will use the 'Pclass' and 'Sex' columns as the index because they allow us to group passengers based on their class and gender, and we want to analyze how the fare varies between these groups. By using 'Fare' as the values, we can see the average fare for each group of passengers, which helps us compare the fare across different classes and genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Fare\n",
      "Pclass Sex               \n",
      "1      female  106.125798\n",
      "       male     67.226127\n",
      "2      female   21.970121\n",
      "       male     19.741782\n",
      "3      female   16.118810\n",
      "       male     12.661633\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = pd.read_csv('https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/titanic.csv')\n",
    "\n",
    "# Pivot the data using the 'Pclass' and 'Sex' columns as the index, and the 'Fare' column as the values\n",
    "pivot_1 = titanic.pivot_table(index=['Pclass', 'Sex'], values='Fare')\n",
    "\n",
    "# Preview the resulting pivot table\n",
    "print(pivot_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform multi-level indexing using the 'pivot_table()' method.\n",
    "\n",
    "In the code below, we selected the 'Pclass' and 'Embarked' columns as the index because they categorize the passengers based on their class and port of embarkation, which can provide insights into how age and fare prices vary for different groups of passengers. We set 'Age' and 'Fare' as the values and applied various aggregation functions to calculate statistics such as mean, median, and standard deviation for both variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Age                          Fare                    \n",
      "                      mean median        std        mean   median        std\n",
      "Pclass Embarked                                                             \n",
      "1      C         38.027027   36.5  14.243454  104.718529  78.2667  99.093935\n",
      "       Q         38.500000   38.5   7.778175   90.000000  90.0000   0.000000\n",
      "       S         38.152037   37.0  15.315584   70.364862  52.0000  58.811278\n",
      "2      C         22.766667   25.0  10.192551   25.358335  24.0000  11.345067\n",
      "       Q         43.500000   43.5  19.091883   12.350000  12.3500   0.000000\n",
      "       S         30.386731   30.0  14.080001   20.327439  13.5000  13.630741\n",
      "3      C         20.741951   20.0  11.712367   11.214083   7.8958   4.871528\n",
      "       Q         25.937500   21.5  16.807938   11.183393   7.7500   6.721677\n",
      "       S         25.696552   25.0  12.110906   14.644083   8.0500  13.276609\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = pd.read_csv('https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/titanic.csv')\n",
    "\n",
    "# Create a pivot table based on \"Pclass\" and \"Embarked\" columns\n",
    "pivot_table = titanic.pivot_table(index=['Pclass', 'Embarked'], values=['Age', 'Fare'], aggfunc={'Age': ['mean', 'median', 'std'], 'Fare': ['mean', 'median', 'std']})\n",
    "\n",
    "# Print the pivot table\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting pivot table shows the average age and fare prices, along with their respective statistics, for each combination of 'Pclass' and 'Embarked'. This allows us to compare the age and fare prices across different classes and ports of embarkation.\n",
    "\n",
    "## Melting data\n",
    "Melting data is a technique used to restructure a dataset from a wide format to a long format. In the wide format, the dataset has multiple columns for each variable, and each observation is represented by a single row. For example, in a dataset of students' exam scores, each row could represent a single student, and each column could represent the scores for a particular subject. In the long format, each variable is represented by a single column, and the observations are spread across multiple rows. By reshaping the data in this way, it becomes easier to compare and analyze multiple variables.\n",
    "\n",
    "Comparing survival rates by Gender: You can melt the dataset to have 'PassengerId' as the identifier variable and 'Sex' and 'Survived' as the melted variables. This will give you a long format dataset with a row for each passenger-gender combination, and columns for 'PassengerId', 'Sex', and 'Survived'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    PassengerId variable   value\n",
      "0             1      Sex    male\n",
      "1             2      Sex  female\n",
      "2             3      Sex  female\n",
      "3             4      Sex  female\n",
      "4             5      Sex    male\n",
      "..          ...      ...     ...\n",
      "95           96      Sex    male\n",
      "96           97      Sex    male\n",
      "97           98      Sex    male\n",
      "98           99      Sex  female\n",
      "99          100      Sex    male\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = pd.read_csv('https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/titanic.csv')\n",
    "\n",
    "# Select the column we want to keep as \"identifier\" variable\n",
    "id_vars = [\"PassengerId\"]\n",
    "\n",
    "# Select the columns we want to melt into the long format\n",
    "value_vars = [\"Sex\", \"Survived\"]\n",
    "\n",
    "# Melt the dataset to go from a wide format to a long format\n",
    "df_melted = pd.melt(titanic, id_vars=id_vars, value_vars=value_vars, var_name=\"variable\", value_name=\"value\")\n",
    "\n",
    "# Print the first few rows of the melted dataset\n",
    "print(df_melted.head(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'pd.melt()' function, the 'id_vars' parameter is used to specify which column(s) to keep as identifier variables, while the 'value_vars' parameter is used to specify which columns to melt into the long format. The 'var_name' and 'value_name' parameters are used to specify the names of the new columns created in the melted dataset. 'var_name' specifies the column containing variable names in the long format, while 'value_name' renames the column containing variable values.\n",
    "\n",
    "### NOTE\n",
    "\n",
    "Melting data from a wide format to a long format can be a useful technique for restructuring datasets and making them easier to analyze. However, one potential issue to consider is that melting the data may lead to an increase in the size of the dataset. This is because each row in the original wide format may correspond to multiple rows in the melted long format, resulting in a significant increase in the number of rows. As a result, it is important to be aware of the potential impact on computational efficiency when using this technique.\n",
    "\n",
    "## Stacking and unstacking data\n",
    "Stacking and unstacking refer to the process of converting the data representation from one format to another. Stacking is the process of pivoting the data from a wide format to a long format, where each column becomes a single row. Unstacking, on the other hand, is the process of pivoting the data from a long format to a wide format, where each row becomes a single column.\n",
    "\n",
    "Suppose you want to analyze the relationship between passenger class, port of embarkation, and survival on the Titanic disaster. To do this, you will first group the Titanic data by passenger class, port of embarkation, and survival.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Pclass Embarked  Survived  count\n",
      "0        1        C         0     26\n",
      "1        1        C         1     59\n",
      "2        1        Q         0      1\n",
      "3        1        Q         1      1\n",
      "4        1        S         0     53\n",
      "5        1        S         1     74\n",
      "6        2        C         0      8\n",
      "7        2        C         1      9\n",
      "8        2        Q         0      1\n",
      "9        2        Q         1      2\n",
      "10       2        S         0     88\n",
      "11       2        S         1     76\n",
      "12       3        C         0     41\n",
      "13       3        C         1     25\n",
      "14       3        Q         0     45\n",
      "15       3        Q         1     27\n",
      "16       3        S         0    286\n",
      "17       3        S         1     67 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = pd.read_csv('https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/titanic.csv')\n",
    "\n",
    "# Group the data by \"Pclass\", \"Embarked\", and \"Survived\"\n",
    "grouped_data = titanic.groupby([\"Pclass\", \"Embarked\", \"Survived\"]).size().reset_index(name=\"count\")\n",
    "print(grouped_data,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the given code, 'reset_index()' is used to reset the index of the 'grouped_data' DataFrame after it has been grouped and counted by the 'Pclass', 'Embarked', and 'Survived' columns.\n",
    "\n",
    "Now, the 'pivot_table()' method uses  'Pclass' as the index and 'Embarked' and 'Survived' as the columns. Then the 'stack()' method is used to reshape the pivot table into a long format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embarked   C       Q        S    \n",
      "Survived   0   1   0   1    0   1\n",
      "Pclass                           \n",
      "1         26  59   1   1   53  74\n",
      "2          8   9   1   2   88  76\n",
      "3         41  25  45  27  286  67 \n",
      "\n",
      "Embarked  Pclass  Survived   C   Q    S\n",
      "0              1         0  26   1   53\n",
      "1              1         1  59   1   74\n",
      "2              2         0   8   1   88\n",
      "3              2         1   9   2   76\n",
      "4              3         0  41  45  286\n",
      "5              3         1  25  27   67\n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "titanic = pd.read_csv('https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/titanic.csv')\n",
    "\n",
    "# Group the data by \"Pclass\", \"Embarked\", and \"Survived\"\n",
    "grouped_data = titanic.groupby([\"Pclass\", \"Embarked\", \"Survived\"]).size().reset_index(name=\"count\")\n",
    "\n",
    "# Pivot the data using \"unstack\" to create columns for each embarkation port\n",
    "pivot_table = grouped_data.pivot_table(index=\"Pclass\", columns=[\"Embarked\", \"Survived\"], values=\"count\")\n",
    "print(pivot_table,\"\\n\")\n",
    "\n",
    "# Use the \"stack\" method to reshape the data into a long format\n",
    "# The \"reset_index\" method is used to reset the index of the reshaped_data DataFrame\n",
    "reshaped_data = pivot_table.stack().reset_index()\n",
    "\n",
    "# Print the reshaped data\n",
    "print(reshaped_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame provides a summarized representation of the 'grouped_data' based on the 'Pclass', 'Embarked', and 'Survived' columns.\n",
    "\n",
    "## Transposing data\n",
    "In Pandas, transposing a dataframe involves flipping the rows and columns of the original dataframe, so that the rows become columns and the columns become rows. This can be achieved using the '.T' attribute or the 'transpose()' method of the dataframe.\n",
    "\n",
    "Here is the example using the '.T' attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   0  \\\n",
      "PassengerId                        1   \n",
      "Age                             22.0   \n",
      "Name         Braund, Mr. Owen Harris   \n",
      "Sex                             male   \n",
      "\n",
      "                                                             1  \\\n",
      "PassengerId                                                  2   \n",
      "Age                                                       38.0   \n",
      "Name         Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
      "Sex                                                     female   \n",
      "\n",
      "                                  2  \\\n",
      "PassengerId                       3   \n",
      "Age                            26.0   \n",
      "Name         Heikkinen, Miss. Laina   \n",
      "Sex                          female   \n",
      "\n",
      "                                                        3                 4  \\\n",
      "PassengerId                                             4                 6   \n",
      "Age                                                  35.0               NaN   \n",
      "Name         Futrelle, Mrs. Jacques Heath (Lily May Peel)  Moran, Mr. James   \n",
      "Sex                                                female              male   \n",
      "\n",
      "                                   5                               6  \\\n",
      "PassengerId                        7                               8   \n",
      "Age                             54.0                             2.0   \n",
      "Name         McCarthy, Mr. Timothy J  Palsson, Master. Gosta Leonard   \n",
      "Sex                             male                            male   \n",
      "\n",
      "                                               7  \\\n",
      "PassengerId                                   10   \n",
      "Age                                         14.0   \n",
      "Name         Nasser, Mrs. Nicholas (Adele Achem)   \n",
      "Sex                                       female   \n",
      "\n",
      "                                           8  \n",
      "PassengerId                               11  \n",
      "Age                                      4.0  \n",
      "Name         Sandstrom, Miss. Marguerite Rut  \n",
      "Sex                                   female  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the \"passengers_personal_info.csv\" file\n",
    "passengers_personal_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/passengers_personal_info.csv\")\n",
    "\n",
    "# Transpose the dataframe\n",
    "titanic_transposed = passengers_personal_info.T\n",
    "\n",
    "# Preview the transposed dataframe\n",
    "print(titanic_transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the example using the 'transpose()' method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     0         1                 2       3       4        5  \\\n",
      "PassengerId          1         2                 3       5       6        7   \n",
      "Survived             0         1                 1       0       0        0   \n",
      "Pclass               3         1                 3       3       3        1   \n",
      "SibSp                1         1                 0       0       0        0   \n",
      "Parch                0         0                 0       0       0        0   \n",
      "Ticket       A/5 21171  PC 17599  STON/O2. 3101282  373450  330877    17463   \n",
      "Fare              7.25   71.2833             7.925    8.05  8.4583  51.8625   \n",
      "Cabin              NaN       C85               NaN     NaN     NaN      E46   \n",
      "Embarked             S         C                 S       S       Q        S   \n",
      "\n",
      "                  6        7        8  \n",
      "PassengerId       8        9       11  \n",
      "Survived          0        1        1  \n",
      "Pclass            3        3        3  \n",
      "SibSp             3        0        1  \n",
      "Parch             1        2        1  \n",
      "Ticket       349909   347742  PP 9549  \n",
      "Fare         21.075  11.1333     16.7  \n",
      "Cabin           NaN      NaN       G6  \n",
      "Embarked          S        S        S  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Read the \"travel_info.csv\" file\n",
    "travel_info = pd.read_csv(\"https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/travel_info.csv\")\n",
    "\n",
    "# Transpose the dataframe\n",
    "titanic_transposed = travel_info.transpose()\n",
    "\n",
    "# Preview the transposed dataframe\n",
    "print(titanic_transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFO\n",
    "\n",
    "The '.T' attribute and 'transpose()' method of a Pandas dataframe can be used interchangeably to transpose the rows and columns of a dataset. However, while '.T' is an attribute of the dataframe object, 'transpose()' is a method that can be called on the dataframe object. Additionally, 'transpose()' allows for additional parameters to be passed, such as whether to preserve the index and column names or reset them.\n",
    "\n",
    "\n",
    "## Encoding categorical data\n",
    "Encoding is an essential step in data transformation as it helps to convert categorical variables into a numerical representation that can be processed by machine learning algorithms. Categorical variables are variables that represent discrete values and can take on a limited number of possible values. Examples of categorical variables include gender and education level.\n",
    "\n",
    "To ensure compatibility with most machine learning algorithms, numerical data is often used as input. Therefore, converting categorical variables into a numerical format is a crucial step. The selection of an appropriate encoding technique depends on the specific type of data and the machine learning algorithm utilized.\n",
    "\n",
    "Some of the most common encoding techniques include:\n",
    "\n",
    "* One-hot encoding\n",
    "* Count encoding\n",
    "## One-hot encoding\n",
    "One-hot encoding is a technique that creates a binary column for each category of a categorical variable. This encoding is suitable for nominal categorical variables where the order of categories does not matter. In the Titanic dataset, the 'Sex' and 'Embarked' variables are nominal variables, and we can use one-hot encoding to encode their categories.\n",
    "\n",
    "Here is the code for performing one-hot encoding on the 'Sex' and 'Embarked' variables in the Titanic dataset using the 'get_dummies()' function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "5            6         0       3   \n",
      "6            7         0       1   \n",
      "7            8         0       3   \n",
      "8            9         1       3   \n",
      "9           10         1       2   \n",
      "\n",
      "                                                Name   Age  SibSp  Parch  \\\n",
      "0                            Braund, Mr. Owen Harris  22.0      1      0   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0      1      0   \n",
      "2                             Heikkinen, Miss. Laina  26.0      0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0      1      0   \n",
      "4                           Allen, Mr. William Henry  35.0      0      0   \n",
      "5                                   Moran, Mr. James   NaN      0      0   \n",
      "6                            McCarthy, Mr. Timothy J  54.0      0      0   \n",
      "7                     Palsson, Master. Gosta Leonard   2.0      3      1   \n",
      "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  27.0      0      2   \n",
      "9                Nasser, Mrs. Nicholas (Adele Achem)  14.0      1      0   \n",
      "\n",
      "             Ticket     Fare Cabin  Sex_female  Sex_male  Embarked_C  \\\n",
      "0         A/5 21171   7.2500   NaN           0         1           0   \n",
      "1          PC 17599  71.2833   C85           1         0           1   \n",
      "2  STON/O2. 3101282   7.9250   NaN           1         0           0   \n",
      "3            113803  53.1000  C123           1         0           0   \n",
      "4            373450   8.0500   NaN           0         1           0   \n",
      "5            330877   8.4583   NaN           0         1           0   \n",
      "6             17463  51.8625   E46           0         1           0   \n",
      "7            349909  21.0750   NaN           0         1           0   \n",
      "8            347742  11.1333   NaN           1         0           0   \n",
      "9            237736  30.0708   NaN           1         0           1   \n",
      "\n",
      "   Embarked_Q  Embarked_S  \n",
      "0           0           1  \n",
      "1           0           0  \n",
      "2           0           1  \n",
      "3           0           1  \n",
      "4           0           1  \n",
      "5           1           0  \n",
      "6           0           1  \n",
      "7           0           1  \n",
      "8           0           1  \n",
      "9           0           0  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd  \n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_data = pd.read_csv('https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/titanic.csv')\n",
    "\n",
    "# Encode 'Sex' and 'Embarked' variables using One-Hot Encoding\n",
    "encoded_data = pd.get_dummies(titanic_data, columns=['Sex', 'Embarked'])\n",
    "\n",
    "# Print the encoded data\n",
    "print(encoded_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the 'get_dummies()' function takes the original dataset ('titanic_data') and specifies the columns to be one-hot encoded ('Sex' and 'Embarked') using the 'columns' parameter. It then creates a new dataset ('encoded_data') where each category in the specified columns is replaced with a new binary value.\n",
    "\n",
    "## Count encoding\n",
    "Count encoding is a technique that replaces each category of a categorical variable with the count of its occurrences in the dataset. In the Titanic dataset, the 'Embarked' variable is a nominal variable, and we can use count encoding to encode its categories.\n",
    "\n",
    "Here's the code for count encoding the 'Embarked'  column in the Titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "5            6         0       3   \n",
      "6            7         0       1   \n",
      "7            8         0       3   \n",
      "8            9         1       3   \n",
      "9           10         1       2   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "5                                   Moran, Mr. James    male   NaN      0   \n",
      "6                            McCarthy, Mr. Timothy J    male  54.0      0   \n",
      "7                     Palsson, Master. Gosta Leonard    male   2.0      3   \n",
      "8  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)  female  27.0      0   \n",
      "9                Nasser, Mrs. Nicholas (Adele Achem)  female  14.0      1   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  Embarked_count  \n",
      "0      0         A/5 21171   7.2500   NaN        S           644.0  \n",
      "1      0          PC 17599  71.2833   C85        C           168.0  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S           644.0  \n",
      "3      0            113803  53.1000  C123        S           644.0  \n",
      "4      0            373450   8.0500   NaN        S           644.0  \n",
      "5      0            330877   8.4583   NaN        Q            77.0  \n",
      "6      0             17463  51.8625   E46        S           644.0  \n",
      "7      1            349909  21.0750   NaN        S           644.0  \n",
      "8      2            347742  11.1333   NaN        S           644.0  \n",
      "9      0            237736  30.0708   NaN        C           168.0  \n"
     ]
    }
   ],
   "source": [
    "# Import the Pandas library\n",
    "import pandas as pd  \n",
    "\n",
    "# Load the Titanic dataset\n",
    "titanic_data = pd.read_csv('https://staticasssets.blob.core.windows.net/open-ai-coderunner/scripts/titanic.csv')\n",
    "\n",
    "# Compute the count of each category in the 'Embarked' column\n",
    "embarked_count = titanic_data.groupby('Embarked').size()\n",
    "\n",
    "# Map the count values to the corresponding categories\n",
    "titanic_data['Embarked_count'] = titanic_data['Embarked'].map(embarked_count)\n",
    "\n",
    "# Print the encoded data\n",
    "print(titanic_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we compute the count of each category in the 'Embarked' column using the 'groupby()' method of the Pandas DataFrame. The resulting count values are then mapped to the corresponding categories using the 'map()' method. Finally, we add the resulting count-encoded column 'Embarked_count' to the original dataset.\n",
    "\n",
    "### NOTE\n",
    "\n",
    "One-hot encoding creates a new binary value for each category in the variable, resulting in a large number of new features. Count encoding, on the other hand, replaces each category with a single count value. When selecting an encoding technique, it is important to consider the characteristics of the categorical variable and the requirements of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
