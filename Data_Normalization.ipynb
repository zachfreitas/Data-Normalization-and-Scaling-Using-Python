{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Data normalization and scaling are techniques used to adjust the values of a dataset so that they are on a similar scale and have the same properties. This is important because many statistical and machine learning algorithms assume that the data is normally distributed, or that the features are on a similar scale. If this assumption is not met, the algorithms may produce biased or inaccurate results. \n",
    "\n",
    "By normalizing or scaling the data, the data is transformed into a consistent and interpretable form that is suitable for further analysis and modeling. This is an important step in the data science process that helps to ensure the validity and accuracy of the results obtained from any further analysis or modeling.\n",
    "\n",
    "\n",
    "In this course, we will cover the following topics:\n",
    "\n",
    "I. Data normalization: Transforming the values of a dataset so that they are in a specific range, usually between 0 and 1.\n",
    "II. Data scaling: Transforming the numerical values of a dataset to a specific range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "In this module, the learners will:\n",
    "\n",
    "* Apply normalization and scaling techniques to datasets\n",
    "* Choose the appropriate technique between normalization and scaling, depending on the dataset\n",
    "* Identify datasets that need to be scaled and/or normalized before proceeding with analysis\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is data normalization?\n",
    "Data normalization is the process of transforming the values of a dataset so that they are in a specific range, usually between 0 and 1. Data normalization is often used when the scale of the features in a dataset varies significantly.\n",
    "\n",
    "## Why is it important?\n",
    "Data normalization is important in the context of data processing and analysis because it helps to ensure that the features of a dataset are on a similar scale and have similar properties. For example, consider a dataset that has features that represent different units of measurement, such as height in inches and weight in pounds. If the scale of these features is not normalized, the results of a machine learning algorithm may be biased towards the feature with the larger scale, since it will have a greater influence on the outcome.\n",
    "\n",
    "Normalization helps to mitigate this issue by transforming the values of the features so that they are on a similar scale, usually between 0 and 1. This makes it possible to compare the features in a consistent and interpretable way and also enables us to apply algorithms that assume our data is on a consistent scale.\n",
    "\n",
    "Normalization also helps to handle cases where the scale of the features in a dataset varies significantly. For example, if a feature represents a range of values that is much larger than that of another feature, normalizing the data can help to ensure that the influence of each feature on the analysis or modeling is proportional to its importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-score\n",
    "Z-score normalization, also known as standardization, is a technique for transforming data by subtracting the mean and dividing by the standard deviation of the data. This results in a new dataset with a mean of 0 and a standard deviation of 1. Here's an example of how to apply z-score normalization in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.41421356]\n",
      " [-0.70710678]\n",
      " [ 0.        ]\n",
      " [ 0.70710678]\n",
      " [ 1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Apply z-score normalization\n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(data.reshape(-1, 1)) #The data needs to be reshaped to a 2d array in order for fit_transform to work\n",
    "\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values of the data have been transformed to their z-scores, with a mean of 0 and a standard deviation of 1. We can now use these normalized values for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit vector\n",
    "Unit vector normalization, also known as vector normalization or vector scaling, is a technique used to transform a vector into a unit vector, which has a magnitude of 1. This is achieved by dividing each component of the vector by its length or magnitude.\n",
    "\n",
    "Unit vector normalization is important for data wrangling because it enables us to compare vectors with different magnitudes on the same scale. In machine learning, it is often used as a preprocessing step for algorithms that are sensitive to the scale of the input features, such as K-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "\n",
    "Here is an example of how to apply unit vector normalization in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4472136  0.89442719]\n",
      " [0.6        0.8       ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Apply unit vector normalization\n",
    "normalizer = Normalizer()\n",
    "normalized_data = normalizer.fit_transform(data)\n",
    "\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each row of the data has been transformed into a unit vector (i.e., a vector with a length of 1). Each row of the 'normalized_data' array is a unit vector because the 'Normalizer' class in scikit-learn applies L2 normalization to each sample in the input data by default.\n",
    "\n",
    "L2 normalization (also known as Euclidean normalization) involves dividing each element of a vector by its L2 norm or magnitude. The L2 norm of a vector x with n elements is defined as the square root of the sum of the squared values of its elements. By dividing each element of a vector by its L2 norm, we can transform the vector into a unit vector with a length or magnitude of 1. That is, we are scaling the vector so that it points in the same direction but has a magnitude of 1. We can now use these normalized values for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean subtraction\n",
    "Mean normalization is a technique for transforming data by subtracting the mean from each data point. This results in a new dataset with a mean of 0.\n",
    "\n",
    "Here's an example of how to apply mean normalization in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2. -1.  0.  1.  2.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Calculate the mean of the data\n",
    "mean = np.mean(data)\n",
    "\n",
    "# Apply mean normalization\n",
    "normalized_data = data - mean\n",
    "\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values of the data have been transformed to have a mean of 0. We can now use these normalized values for further analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile\n",
    "Quantile normalization is a technique for transforming data by making the distribution of the data identical to a reference distribution. This is achieved by sorting the values of each column of the data and then replacing each value with the corresponding value from the reference distribution.\n",
    "\n",
    "Here's an example of how to apply quantile normalization in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      " [[10 20 30]\n",
      " [40 50 60]\n",
      " [70 80 90]] \n",
      "\n",
      "Reference distribution:\n",
      " [[1 2 3]\n",
      " [1 2 3]\n",
      " [1 2 3]] \n",
      "\n",
      "Normalized data:\n",
      " [[1 2 3]\n",
      " [1 2 3]\n",
      " [1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.array([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\n",
    "\n",
    "# Create a reference distribution by computing the average ranks of each column\n",
    "reference_distribution = np.mean(rankdata(data, axis=0), axis=1)\n",
    "\n",
    "# Sort the values of each column\n",
    "sorted_data = np.sort(data, axis=0)\n",
    "\n",
    "# Replace each value with the corresponding value from the reference distribution\n",
    "quantile_normalized_data = np.zeros_like(sorted_data)\n",
    "for i in range(sorted_data.shape[1]):\n",
    "    quantile_normalized_data[:, i] = reference_distribution[i]\n",
    "\n",
    "# Invert the sorting of each column to get the original order\n",
    "quantile_normalized_data = np.sort(quantile_normalized_data, axis=0)\n",
    "quantile_normalized_data = np.flip(quantile_normalized_data, axis=0)\n",
    "\n",
    "print(\"Original data:\\n\",data,\"\\n\")\n",
    "print(\"Reference distribution:\\n\",quantile_normalized_data,\"\\n\")\n",
    "print(\"Normalized data:\\n\",quantile_normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the distribution of the data has been transformed to be identical to the reference distribution. We can now use these normalized values for further analysis or modeling. Note that this method works best for datasets with a similar distribution across columns and that it can be computationally expensive for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
