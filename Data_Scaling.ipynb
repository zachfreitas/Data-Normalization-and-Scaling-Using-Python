{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is data scaling?\n",
    "Data scaling is the process of transforming the numerical values of a dataset to a specific range, typically to make them more manageable for analysis or modeling. It involves rescaling the data by applying a linear or nonlinear function to map the data values to a new range. The most common scaling techniques include min-max scaling and logarithmic scaling.\n",
    "\n",
    "# Why is it important?\n",
    "Data scaling is important because it can help improve the accuracy and efficiency of data analysis or modeling. In many cases, datasets may contain features with different scales or units, making it difficult to compare or analyze them together. Scaling can help to eliminate this issue by rescaling the values of each feature to a common range, which can help to improve the performance of many machine learning algorithms. Additionally, scaling can help to reduce the influence of outliers and improve the convergence rate of gradient-based optimization algorithms.\n",
    "\n",
    "For example, consider a dataset that contains both temperature measurements in degrees Celsius and wind speed measurements in kilometers per hour. If these features are used to train a machine learning model, the model may be biased towards the features with larger scales, such as wind speed, and ignore the smaller-scale features, such as temperature.\n",
    "\n",
    "# Logarithmic\n",
    "Logarithmic scaling is a technique for transforming data by taking the logarithm of the values in a given dataset. This can help to reduce the impact of very large or very small values and can make the data more manageable for analysis or modeling. Here's an example of how to apply logarithmic scaling in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.array([1, 10, 100, 1000, 10000])\n",
    "\n",
    "# Apply logarithmic scaling\n",
    "log_data = np.log10(data)\n",
    "print(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking log with base 10 isn't the only option, we can also take a natural logarithm in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         2.30258509 4.60517019 6.90775528 9.21034037]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset\n",
    "data = np.array([1, 10, 100, 1000, 10000])\n",
    "\n",
    "# Apply logarithmic scaling\n",
    "log_data = np.log(data)\n",
    "\n",
    "print(log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the logarithm of each element, we can reduce the range of values in the dataset and make it easier to visualize or analyze. In this particular case, taking the logarithm of data has compressed the values so that they are all within a few orders of magnitude of each other, making it easier to compare and analyze them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decimal\n",
    "Decimal scaling is a technique for scaling data by shifting the decimal point of the values in a dataset. This can help to make the values more manageable and easier to work with. Here's an example of how to apply decimal scaling in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1e-05, 0.0001, 0.001, 0.01, 0.1]\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataset\n",
    "data = [10, 100, 1000, 10000, 100000]\n",
    "\n",
    "# Find the number of digits in the maximum value\n",
    "max_digits = len(str(max(data)))\n",
    "\n",
    "# Calculate the scaling factor\n",
    "scale_factor = 10 ** max_digits\n",
    "\n",
    "# Apply decimal scaling\n",
    "scaled_data = [x / scale_factor for x in data]\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values in the original dataset have been scaled by shifting the decimal point to the left by the maximum number of digits in any of the values. This can help to make the data more manageable and easier to work with, particularly for machine learning models that require input features to be on a similar scale. Note that it's important to choose an appropriate scaling factor based on the characteristics of the data and the intended use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust\n",
    "Robust scaling, also known as median and interquartile range (IQR) scaling, is a technique for scaling data based on the median and interquartile range of the values in a given dataset. This can help to reduce the impact of outliers and make the data more robust to extreme values. Here's an example of how to apply robust scaling in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02862986]\n",
      " [-0.0204499 ]\n",
      " [-0.01226994]\n",
      " [-0.00408998]\n",
      " [ 0.00408998]\n",
      " [ 0.78118609]\n",
      " [ 1.599182  ]\n",
      " [ 2.41717791]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with outliers\n",
    "data = np.array([1, 2, 3, 4, 5, 100, 200, 300])\n",
    "\n",
    "# Create a RobustScaler object\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Apply robust scaling to the data\n",
    "scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values in the original dataset have been transformed using the median and interquartile range to reduce the impact of the extreme values (100, 200, and 300). The resulting scaled values are more robust and less affected by outliers, making them more suitable for analysis or modeling. \n",
    "\n",
    "A potential use case for robust scaling is in financial data analysis, where there may be significant outliers due to extreme market events or other factors. By using robust scaling, we can reduce the impact of these outliers and obtain a more accurate picture of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean absolute deviation\n",
    "MAD (Median Absolute Deviation) scaling, also known as Max Absolute scaling, is a technique for scaling data based on the median absolute deviation of the values in a given dataset. This can help to reduce the impact of outliers and make the data more robust to extreme values. Here's an example of how to apply MAD scaling in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00333333]\n",
      " [0.00666667]\n",
      " [0.01      ]\n",
      " [0.01333333]\n",
      " [0.01666667]\n",
      " [0.33333333]\n",
      " [0.66666667]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample dataset with outliers\n",
    "data = np.array([1, 2, 3, 4, 5, 100, 200, 300])\n",
    "\n",
    "# Create a MaxAbsScaler object\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Apply MAD scaling to the data\n",
    "scaled_data = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the values in the original dataset have been transformed using the median absolute deviation to reduce the impact of the extreme values (100, 200, and 300). The resulting scaled values are more robust and less affected by outliers, making them more suitable for analysis or modeling. \n",
    "\n",
    "A potential use case for MAD scaling is in image processing, where there may be extreme pixel values that need to be normalized in order to improve the performance of image analysis algorithms. By using MAD scaling, we can reduce the impact of these extreme pixel values and obtain more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
