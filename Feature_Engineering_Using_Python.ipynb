{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "In the realm of machine learning, effective data utilization and feature representation play pivotal roles in model performance. This course delves into two fundamental concepts: Data Augmentation and Feature Engineering.\n",
    "\n",
    "Data Augmentation involves diversifying a training dataset by applying transformations to existing data. Common techniques like data annotation, text annotation, and synthetic data generation are covered in this course. For instance, in sentiment analysis, a limited set of customer reviews can be enriched by generating new instances through these transformations. This not only expands the dataset but also enhances the model's ability to generalize.\n",
    "\n",
    "Feature Engineering focuses on refining raw data into informative features, crucial for accurate model predictions. In a scenario predicting customer churn from transaction data, feature engineering might involve creating new features like purchase frequency and total spending over time. These augmented features provide the model with richer information, significantly improving its predictive capabilities. \n",
    "\n",
    "In this module, we will cover the following topics:\n",
    "\n",
    "I. Data augmentation: It involves diversifying a training dataset by applying transformations to existing data.\n",
    "II. Feature engineering: It focuses on refining raw data into informative features, crucial for accurate model predictions.\n",
    "\n",
    "## Learning Objectives\n",
    "In this module, the learners will:\n",
    "\n",
    "Improve raw data by enriching it with additional information\n",
    "Create new useful features using feature engineering techniques\n",
    "Understand why data enrichment is important for getting accurate insights\n",
    "Let's get started!\n",
    "\n",
    "## Dataset\n",
    "Breast Cancer dataset: This is a widely used dataset in the field of medical research and machine learning. This dataset provides information about breast cancer patients, including their demographic information, medical history, and the characteristics of their tumors. \n",
    "\n",
    "In terms of data cleaning, the dataset may contain missing values or erroneous data that need to be addressed before any analysis can be performed. Some of the common data cleaning techniques that can be applied to this dataset include identifying and handling missing values, checking for duplicates, and removing any irrelevant columns.\n",
    "\n",
    "The columns in this dataset are:\n",
    "\n",
    "* patient_id: This column contains a unique identification number assigned to each patient in the dataset.\n",
    "* clump_thickness: This column represents the thickness of the tumor in the range of 1 to 10.\n",
    "* cell_size_uniformity: This column represents the uniformity in size of tumor cells in the range of 1 to 10.\n",
    "* cell_shape_uniformity: This column represents the uniformity in the shape of tumor cells in the range of 1 to 10.\n",
    "* marginal_adhesion: This column represents the level of adhesion of tumor cells to the surrounding tissue in the range of 1 to 10.\n",
    "* single_ep_cell_size: This column represents the size of the tumor's epithelial cells in the range of 1 to 10.\n",
    "* bare_nuclei: This column represents the presence or absence of a nucleus in the tumor cells. It contains values ranging from 1 to 10, where 1 represents the absence of a nucleus and 10 represents the presence of a nucleus.\n",
    "* bland_chromatin: This column represents the uniformity of the chromatin material within the tumor cells, ranging from 1 to 10.\n",
    "* normal_nucleoli: This column represents the normalcy of the nucleoli within the tumor cells, ranging from 1 to 10.\n",
    "* mitoses: This column represents the level of mitosis (cell division) within the tumor cells, ranging from 1 to 10.\n",
    "* class: This column contains the diagnosis of the tumor as either \"benign\" or \"malignant\".\n",
    "* doctor_name: This column contains the name of the doctor who diagnosed the tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "## What is data augmentation?\n",
    "Data augmentation is a technique used in machine learning and computer vision to increase the size and diversity of a training dataset by creating new variations of existing data. It involves applying various transformations to the original data, such as rotation, scaling, flipping, cropping, or adding noise, to generate new data instances that are similar but not identical to the original ones. \n",
    "\n",
    "Suppose you have a dataset of customer reviews for a product, and you want to train a sentiment analysis model to classify the reviews as positive, negative, or neutral. However, the dataset contains a limited number of examples, and some of the reviews are very short or contain typos or grammatical errors, making it challenging for the model to learn meaningful patterns. By applying these data augmentation techniques, you can generate new instances of the original reviews that are similar but not identical to the original ones. This can help increase the training set's size and diversity, improve the model's ability to generalize to new reviews, and reduce the risk of overfitting the original data.\n",
    "\n",
    "## Why is it important?\n",
    "Data augmentation is an important technique in data enrichment that helps improve the performance of machine learning models. It is particularly important when working with small datasets or when the available data is not representative enough of the real-world scenarios that the model is expected to encounter. By artificially expanding the size of a dataset through data augmentation, data scientists can create more diverse and robust datasets that are better able to capture the complexity and variability of real-world data.\n",
    "\n",
    "Moreover, data augmentation helps reduce overfitting, where the model learns to recognize specific examples in the training data rather than generalizing to new, unseen data. By generating additional data samples, data augmentation ensures that the model learns from a broader range of examples, reducing the risk of overfitting and improving its ability to generalize to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data annotation\n",
    "Data annotation is the process of adding metadata or labels to data, to help machines better understand and analyze it. Annotation can be applied to a wide range of data types, such as text, images, videos, and audio. By providing annotated data to machine learning algorithms, we can train models to recognize and classify new instances of data, leading to improved accuracy and efficiency in a variety of applications. Suppose we have a dataset of customer reviews for a product, and we want to classify each review as either positive or negative. We can annotate the dataset by adding a new column to the Pandas DataFrame that contains the label for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               review_text     label\n",
      "0     The product was good  Positive\n",
      "1  I had a good experience  Positive\n",
      "2      The product was bad  Negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.DataFrame({\n",
    "    \"review_text\":[\"The product was good\", \"I had a good experience\", \"The product was bad\"],\n",
    "})\n",
    "\n",
    "# Define a function to label each review as positive or negative\n",
    "def label_review(text):\n",
    "\n",
    "# perform sentiment analysis or use predefined rules to determine the label\n",
    "# For this example, we will use a simple rule-based approach based on the word 'good'\n",
    "    if 'good' in text.lower():\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "        \n",
    "# Add a new column to the DataFrame that contains the label for each review\n",
    "df['label'] = df['review_text'].apply(label_review)\n",
    "\n",
    "# Save the annotated dataset to a new CSV file\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we define a function 'label_review' that takes a review text as input and returns a label ('Positive' or 'Negative') based on some rule or model. For this example, we use a simple rule-based approach based on the presence of the word 'good' in the review text. We then apply this function to each review text in the DataFrame using the 'apply()' method, and store the result in a new column called 'label'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text augmentation\n",
    "Text augmentation is a technique used in natural language processing (NLP) to increase the amount of training data by creating variations of existing text data. This technique is used to overcome the problem of insufficient training data, which can lead to poor model performance.\n",
    "\n",
    "There are various text augmentation techniques such as replacing words with their synonyms, deleting or adding words, and changing word order. These techniques can be applied randomly or with certain rules to create diverse variations of the original text data.\n",
    "\n",
    "Here's an example of text augmentation code using a custom dictionary-based approach in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The quick brown fox jumps over the lazy dog\n",
      "Augmented text: The rapid hazel jackal leaps over the idle pooch\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def synonym_replacement(text):\n",
    "    word_replacements = {\n",
    "        'quick': ['fast', 'speedy', 'rapid'],\n",
    "        'brown': ['chestnut', 'tawny', 'hazel'],\n",
    "        'fox': ['wildcat', 'vixen', 'jackal'],\n",
    "        'jumps': ['leaps', 'bounds', 'vaults'],\n",
    "        'lazy': ['idle', 'sluggish', 'lethargic'],\n",
    "        'dog': ['canine', 'pooch', 'hound']\n",
    "    }\n",
    "\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_replacements:\n",
    "            synonyms = word_replacements[word]\n",
    "            new_word = random.choice(synonyms)\n",
    "        else:\n",
    "            new_word = word\n",
    "        new_words.append(new_word)\n",
    "\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# Example usage\n",
    "original_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "augmented_text = synonym_replacement(original_text)\n",
    "\n",
    "print(\"Original text:\", original_text)\n",
    "print(\"Augmented text:\", augmented_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the 'synonym_replacement' function takes a text input and replaces each word in the text with one of its synonyms. It uses a dictionary called 'word_replacements' to store predefined replacements for specific words.\n",
    "\n",
    "The function tokenizes the input text by splitting it into individual words. It then iterates over each word and checks if it exists in the 'word_replacements' dictionary. If a replacement is available, it randomly selects a synonym for that word using the 'random.choice()' function. If no replacement is available, it keeps the original word. The function collects the new words in a list and finally joins them back into a single string using the 'join(new_words)' method.\n",
    "\n",
    "The example usage demonstrates the function by providing an original text and obtaining an augmented text with certain words replaced by their synonyms. The original and augmented texts are printed for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data generation\n",
    "Synthetic data generation is the process of creating new artificial data points that have similar statistical properties and characteristics to a given dataset. Synthetic data generation can be useful when we have a limited amount of real data or when we want to generate new data that is representative of a certain population. Synthetic data generation can be a powerful tool for data scientists, as it can help to mitigate issues related to data scarcity and data privacy, and can enable more sophisticated and robust machine learning models. However, it's important to ensure that the synthetic data is representative of the real data and does not introduce any biases or distortions in the analysis.\n",
    "\n",
    "Here's an example of when synthetic data generation might be useful: Suppose we have a dataset of credit card transactions and we want to train a machine learning model to detect fraudulent transactions. However, the dataset contains only a small number of fraudulent transactions, making it difficult to train an accurate model. In this case, we could use synthetic data generation to create new artificial fraudulent transactions that have similar statistical properties to the real fraudulent transactions in the dataset. This can help to improve the accuracy and robustness of the machine learning model. Here's a code sample in Python that demonstrates how to generate synthetic data using the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10) (1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a synthetic dataset with 1000 samples and 10 features\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the 'make_classification()' function from scikit-learn to generate a synthetic dataset with 1000 samples and 10 features. The 'n_samples' argument specifies the number of data points to generate, while the 'n_features' argument specifies the number of features in each data point. The 'random_state' argument is used to ensure the reproducibility of the results. The function returns two arrays: 'X', which contains the feature values for each data point, and 'y', which contains the target labels for each data point. In this case, since the 'make_classification()' function is used to generate synthetic data, we can control the proportion of positive and negative samples in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "## What is feature engineering?\n",
    "Feature engineering is the process of transforming raw data into a set of features that are more informative and relevant for a specific machine learning task. Feature engineering can involve a variety of techniques, such as selecting relevant features, transforming existing features, creating new features, and combining multiple features. The goal of feature engineering is to extract as much useful information from the data as possible and to make the resulting features as predictive as possible for the given task. \n",
    "\n",
    "Suppose we have a dataset of customer transactions, and we want to predict which customers are likely to churn (i.e., stop using our services). The dataset contains information about each transaction, such as the amount spent, the date of the transaction, and the type of product purchased. However, these features alone may not be sufficient to accurately predict churn. In this case, we could use feature engineering to create new features that capture additional information about each customer, such as their frequency of purchases, their total spending over time, and their most common type of product purchased. By creating these new features, we can provide the machine learning model with more relevant information and improve its ability to predict churn.\n",
    "\n",
    "## Why is it important?\n",
    "Feature engineering is a crucial process in data enrichment that involves transforming raw data into a set of features that can be used to train machine learning models. It is a critical step in the data science pipeline that can significantly impact the performance of the final model.\n",
    "\n",
    "Feature engineering helps to extract relevant information from the raw data and represent it in a way that is suitable for machine learning algorithms. By selecting and transforming the most informative features, data scientists can reduce noise and improve the accuracy and generalization of the model.\n",
    "\n",
    "Furthermore, feature engineering can help to address the challenge of high-dimensional data, where the performance of machine learning models deteriorates as the number of features increases. By selecting only the most relevant features and combining them in meaningful ways, data scientists can reduce the number of features without losing essential information, which can improve the model's performance.\n",
    "\n",
    "Feature engineering can also help to handle missing data and outliers, which are common problems in real-world datasets. By imputing missing values or treating outliers in a meaningful way, data scientists can create more robust and representative datasets, which can help to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "Feature selection is a process of selecting a subset of relevant features from a larger set of features in a dataset. The goal of feature selection is to improve the performance of a machine-learning model by reducing the dimensionality of the input feature space and eliminating irrelevant or redundant features. \n",
    "\n",
    "Feature selection is useful in several ways, it can improve the accuracy and performance of a machine learning model by reducing overfitting and improving the model's generalization ability, and it can reduce the computational complexity and memory requirements of the model by reducing the number of input features. Here is an example of feature selection using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  3  6  7 20 22 23 26 27]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# select the k best features using f_classif\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# print the selected feature indices\n",
    "print(selector.get_support(indices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True  True False False  True  True False False False False\n",
      " False False False False False False False False  True False  True  True\n",
      " False False  True  True False False]\n"
     ]
    }
   ],
   "source": [
    "# print the selected feature indices\n",
    "print(selector.get_support(indices=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we load the breast cancer dataset using scikit-learn's 'load_breast_cancer()' function. The dataset contains 30 input features and a binary target variable that represents the diagnosis of breast cancer (malignant or benign).\n",
    "\n",
    "We then use scikit-learn's 'SelectKBest()' class to select the 10 best features using the 'f_classif()' scoring function. This function calculates the ANOVA F-value of each feature with respect to the target variable and selects the k best features based on their F-values.\n",
    "\n",
    "Finally, we transform the original input feature matrix 'X' into a new feature matrix 'X_new' that contains only the selected features. We also print the indices of the selected features using the 'get_support()' method of the selector object.\n",
    "\n",
    "In this example, feature selection helps to reduce the dimensionality of the input feature space and identify the most important features that contribute to the diagnosis of breast cancer. This can improve the accuracy and performance of the machine learning model and make it more interpretable and explainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning\n",
    "Binning is the process of dividing a continuous variable into a discrete set of intervals or bins. It is a form of data discretization that can help to simplify complex data distributions, reduce noise, and make data more interpretable. Binning is useful in several ways, it can help to reduce the sensitivity of a model to outliers and extreme values by grouping them together with other values in the same bin, and it can help to capture non-linear relationships between variables by creating bins that reflect non-linear patterns in the data, and it can help to reduce the dimensionality of the input feature space by discretizing continuous variables into a smaller number of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   mean radius  mean texture\n",
      "0        17.99         10.38\n",
      "1        20.57         17.77\n",
      "2        19.69         21.25\n",
      "3        11.42         20.38\n",
      "4        20.29         14.34\n",
      "\n",
      "Binned Data:\n",
      "  mean radius mean texture\n",
      "0           1            1\n",
      "1           2            1\n",
      "2           1            2\n",
      "3           1            2\n",
      "4           2            1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Define the bin edges\n",
    "bin_edges = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "# Select the columns to bin\n",
    "columns_to_bin = ['mean radius', 'mean texture']\n",
    "\n",
    "# Create a new dataframe with the binned values\n",
    "binned_df = pd.DataFrame()\n",
    "for col in columns_to_bin:\n",
    "\tbinned_df[col] = pd.cut(df[col], bins=bin_edges, labels=range(len(bin_edges)-1))\n",
    "\n",
    "# Print the original and binned dataframes for comparison\n",
    "print(\"Original Data:\")\n",
    "print(df[columns_to_bin].head())\n",
    "print(\"\\nBinned Data:\")\n",
    "print(binned_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads the breast cancer dataset using scikit-learn's 'load_breast_cancer()' function and creates a Pandas dataframe from the data using the feature names provided in the dataset.\n",
    "\n",
    "Next, the code defines the bin edges as a list with ranges. The code then selects the columns to bin by creating a list of the column names. Only the 'mean radius' and 'mean texture' columns are selected in this case.\n",
    "\n",
    "A new empty DataFrame is created, and a for loop is used to iterate over the selected columns. For each column, the 'pd.cut()' function is used to bin the values based on the previously defined bin edges, and the binned values are added to the new DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical variables \n",
    "Encoding categorical variables is the process of converting categorical data into numerical data that can be used in machine learning models. Categorical data represent discrete values that belong to a specific category, such as colors, gender, or country of origin. However, machine learning algorithms typically work with numerical data, so encoding categorical variables is necessary to use this type of data in a machine learning pipeline. Here's an example of how we can encode categorical variables in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  target_names  target_encoded\n",
      "0    malignant               1\n",
      "1       benign               0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Convert the 'target' variable to a Pandas DataFrame\n",
    "target_df = pd.DataFrame(data.target_names, columns=['target_names'])\n",
    "\n",
    "# Encode the 'target' variable using LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "target_df['target_encoded'] = encoder.fit_transform(target_df['target_names'])\n",
    "\n",
    "# Print the encoded target variable\n",
    "print(target_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we first load the breast cancer dataset using the 'load_breast_cancer()' function from the scikit-learn library. We then convert the 'target_names' variable to a Pandas DataFrame and use 'LabelEncoder()' to encode the categorical data into numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derived features\n",
    "Derived features are new features that are created from existing features in a dataset using mathematical operations, domain-specific knowledge, or other techniques. The goal of creating derived features is to improve the performance of machine learning models by providing additional information or reducing noise in the data.\n",
    "\n",
    "Derived features are useful in many machine-learning applications, including image recognition, natural language processing, and financial analysis. For example, in image recognition, derived features such as the texture or shape of an object can be used to improve the accuracy of a model. Here's an example of creating derived features using the breast cancer dataset in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0        17.99         10.38          122.80     1001.0          0.11840   \n",
      "1        20.57         17.77          132.90     1326.0          0.08474   \n",
      "2        19.69         21.25          130.00     1203.0          0.10960   \n",
      "3        11.42         20.38           77.58      386.1          0.14250   \n",
      "4        20.29         14.34          135.10     1297.0          0.10030   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0           0.27760          0.3001              0.14710         0.2419   \n",
      "1           0.07864          0.0869              0.07017         0.1812   \n",
      "2           0.15990          0.1974              0.12790         0.2069   \n",
      "3           0.28390          0.2414              0.10520         0.2597   \n",
      "4           0.13280          0.1980              0.10430         0.1809   \n",
      "\n",
      "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
      "0                 0.07871  ...          17.33           184.60      2019.0   \n",
      "1                 0.05667  ...          23.41           158.80      1956.0   \n",
      "2                 0.05999  ...          25.53           152.50      1709.0   \n",
      "3                 0.09744  ...          26.50            98.87       567.7   \n",
      "4                 0.05883  ...          16.67           152.20      1575.0   \n",
      "\n",
      "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
      "0            0.1622             0.6656           0.7119                0.2654   \n",
      "1            0.1238             0.1866           0.2416                0.1860   \n",
      "2            0.1444             0.4245           0.4504                0.2430   \n",
      "3            0.2098             0.8663           0.6869                0.2575   \n",
      "4            0.1374             0.2050           0.4000                0.1625   \n",
      "\n",
      "   worst symmetry  worst fractal dimension  radius_texture_ratio  \n",
      "0          0.4601                  0.11890              1.733141  \n",
      "1          0.2750                  0.08902              1.157569  \n",
      "2          0.3613                  0.08758              0.926588  \n",
      "3          0.6638                  0.17300              0.560353  \n",
      "4          0.2364                  0.07678              1.414923  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Create a new feature that represents the ratio of mean radius to mean texture\n",
    "df['radius_texture_ratio'] = df['mean radius'] / df['mean texture']\n",
    "\n",
    "# Print the first 5 rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we first load the breast cancer dataset using the 'load_breast_cancer()' function from the scikit-learn library. We then create a Pandas DataFrame from the dataset.\n",
    "\n",
    "Next, we create a new feature called 'radius_texture_ratio' by dividing the 'mean radius' feature by the 'mean texture' feature. This derived feature represents the ratio of the mean radius to the mean texture of a breast cancer cell.\n",
    "\n",
    "In this example, creating a derived feature helps to provide additional information about the relationship between the mean radius and mean texture features. This derived feature can be used as an input feature in machine learning models to improve their accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
